{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQP4bZSyGIJK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./data/train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1h2ogUTGIJO",
    "outputId": "9e110690-66b2-4b12-bdce-c8a808acf019",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>\"How to discriminate oil from gutter oil by me...</td>\n",
       "      <td>It took 30 years of cooking oil to know that o...</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321185</th>\n",
       "      <td>167562</td>\n",
       "      <td>114783</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>西媒曝萨拉赫被推荐至巴萨 经纪人辟谣：并未发生</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>West Media Exposing Tallahlach has been recomm...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321182</th>\n",
       "      <td>167562</td>\n",
       "      <td>114782</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>萨拉赫经纪人辟谣：与巴萨传闻不实</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Salah agent's dishonest rumour: rumour with ba...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321184</th>\n",
       "      <td>167562</td>\n",
       "      <td>137705</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>萨拉赫辟谣传闻：埃及非常团结，我们之间没有任何分歧</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Rumor has it that Egypt is very united and the...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321186</th>\n",
       "      <td>167562</td>\n",
       "      <td>48989</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣不实传闻，埃及足协主席：萨拉赫不会提前离队</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>False rumors, Egypt Football Association Chair...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321188</th>\n",
       "      <td>167563</td>\n",
       "      <td>66480</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆女儿辟谣：萨达姆政权二号人物没死！</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>Saddam's daughter refutes rumors: no. 2 of Sad...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320552 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tid1    tid2                          title1_zh  \\\n",
       "id                                                          \n",
       "0            0       1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "3            2       3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "1            2       4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2            2       5  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "9            6       7               \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "...        ...     ...                                ...   \n",
       "321185  167562  114783    萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321182  167562  114782    萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321184  167562  137705    萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321186  167562   48989    萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321188  167563   66480      萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗   \n",
       "\n",
       "                         title2_zh  \\\n",
       "id                                   \n",
       "0         警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "3        深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "1             GDP首超香港？深圳澄清：还差一点点……   \n",
       "2       去年深圳GDP首超香港？深圳统计局辟谣：还差611亿   \n",
       "9          吃了30年食用油才知道，一片大蒜轻松鉴别地沟油   \n",
       "...                            ...   \n",
       "321185     西媒曝萨拉赫被推荐至巴萨 经纪人辟谣：并未发生   \n",
       "321182            萨拉赫经纪人辟谣：与巴萨传闻不实   \n",
       "321184   萨拉赫辟谣传闻：埃及非常团结，我们之间没有任何分歧   \n",
       "321186     辟谣不实传闻，埃及足协主席：萨拉赫不会提前离队   \n",
       "321188        萨达姆女儿辟谣：萨达姆政权二号人物没死！   \n",
       "\n",
       "                                                title1_en  \\\n",
       "id                                                          \n",
       "0       There are two new old-age insurance benefits f...   \n",
       "3       \"If you do not come to Shenzhen, sooner or lat...   \n",
       "1       \"If you do not come to Shenzhen, sooner or lat...   \n",
       "2       \"If you do not come to Shenzhen, sooner or lat...   \n",
       "9       \"How to discriminate oil from gutter oil by me...   \n",
       "...                                                   ...   \n",
       "321185  egypt 's presidential election failed to win m...   \n",
       "321182  egypt 's presidential election failed to win m...   \n",
       "321184  egypt 's presidential election failed to win m...   \n",
       "321186  egypt 's presidential election failed to win m...   \n",
       "321188  Will the United States wage war on Iraq withou...   \n",
       "\n",
       "                                                title2_en      label  \n",
       "id                                                                    \n",
       "0       Police disprove \"bird's nest congress each per...  unrelated  \n",
       "3       Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "1       The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  \n",
       "2       Shenzhen's GDP topped Hong Kong last year? She...  unrelated  \n",
       "9       It took 30 years of cooking oil to know that o...     agreed  \n",
       "...                                                   ...        ...  \n",
       "321185  West Media Exposing Tallahlach has been recomm...  unrelated  \n",
       "321182  Salah agent's dishonest rumour: rumour with ba...  unrelated  \n",
       "321184  Rumor has it that Egypt is very united and the...  unrelated  \n",
       "321186  False rumors, Egypt Football Association Chair...  unrelated  \n",
       "321188  Saddam's daughter refutes rumors: no. 2 of Sad...  unrelated  \n",
       "\n",
       "[320552 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dmZQSLKGIJS",
    "outputId": "a67124fc-9136-46d5-d736-d448f67170ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh                  title2_zh      label\n",
       "id                                                                         \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated\n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated\n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['title1_zh', \n",
    "        'title2_zh', \n",
    "        'label']\n",
    "train = train.loc[:, cols]\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROicMn1mGIJV",
    "outputId": "786a8972-ca11-43d6-fb65-6daa095e7b37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Meng', 'Lee,', 'a', 'data', 'scientist', 'based', 'in', 'Tokyo.']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am Meng Lee, a data scientist based in Tokyo.'\n",
    "#英文的word segmentation只需要用空白分隔\n",
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdMlb1eOGIJZ"
   },
   "outputs": [],
   "source": [
    "#中文的segmentation用結巴來做\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "text = '我是李孟，在東京工作的數據科學家。'\n",
    "words = pseg.cut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVcjyR3YGIJc",
    "outputId": "6d4ea820-ffa7-443e-882e-d079320baabe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object cut at 0x000001FEBF48C148>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLqdUhykGIJf",
    "outputId": "0f80c48e-4f83-4a36-f3eb-0f2a784c74fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\user10\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.823 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[pair('我', 'r'),\n",
       " pair('是', 'v'),\n",
       " pair('李孟', 'nr'),\n",
       " pair('，', 'x'),\n",
       " pair('在', 'p'),\n",
       " pair('東京', 'ns'),\n",
       " pair('工作', 'vn'),\n",
       " pair('的', 'uj'),\n",
       " pair('數據', 'n'),\n",
       " pair('科學家', 'n'),\n",
       " pair('。', 'x')]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "852J0bmoGIJj"
   },
   "outputs": [],
   "source": [
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([\n",
    "        word for word, flag in words if flag != 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aYmmg6qGIJl"
   },
   "outputs": [],
   "source": [
    "#利用apply將jieba_tokenizer套用到所有新聞標題上\n",
    "#AttributeError: 'float' object has no attribute 'decode'\n",
    "#.astype(str)\n",
    "t1 = pd.DataFrame(train['title1_zh'].astype(str))\n",
    "t2 = pd.DataFrame(train['title2_zh'].astype(str))\n",
    "train['title1_tokenized'] = t1['title1_zh'].apply(jieba_tokenizer)\n",
    "train['title2_tokenized'] = t2['title2_zh'].apply(jieba_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIXHq6dJGIJo",
    "outputId": "5550e4c8-37dd-4df7-c262-1b24819f105d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title1_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh  \\\n",
       "id                                      \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "9                \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                                   title1_tokenized  \n",
       "id                                                   \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗  \n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油  "
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, [0, 3]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kKkvPJsGIJr",
    "outputId": "7c602ed5-dd3d-40bf-b75a-5e48887ed11e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title2_zh                      title2_tokenized\n",
       "id                                                                  \n",
       "0     警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京     警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n",
       "3    深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小    深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n",
       "1         GDP首超香港？深圳澄清：还差一点点……              GDP 首 超 香港 深圳 澄清 还 差 一点点\n",
       "2   去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿\n",
       "9      吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, [1, 4]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhxzHAyMGIJt",
    "outputId": "6d2ae5b9-1a2e-43fe-ca11-2c9644bd9141",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user10\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4nkwFyLGIJx",
    "outputId": "a05ce517-7c07-447e-9db5-ac92cdf02ec0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(641104,)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwYP8dcdGIJ3",
    "outputId": "5476ad99-4a0f-4a6b-c65d-90152d3115ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0                2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
       "3         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "1         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "2         你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "9                              用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
       "                               ...                       \n",
       "321185                   西媒 曝 萨拉 赫 被 推荐 至 巴萨 经纪人 辟谣 并未 发生\n",
       "321182                            萨拉 赫 经纪人 辟谣 与 巴萨 传闻 不 实\n",
       "321184                 萨拉 赫 辟谣 传闻 埃及 非常 团结 我们 之间 没有 任何 分歧\n",
       "321186                   辟谣 不 实 传闻 埃及 足协 主席 萨拉 赫 不会 提前 离队\n",
       "321188                         萨达姆 女儿 辟谣 萨达姆 政权 二号 人物 没 死\n",
       "Length: 641104, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqpmyY0wGIJ9",
    "outputId": "c83d7f06-5dbf-4648-ac8a-b0c1e4771553"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title\n",
       "id                                                 \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpus.iloc[:5],\n",
    "             columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfFvK_lOGIKC"
   },
   "outputs": [],
   "source": [
    "#呼叫 tokenizer 為我們查看所有文本，並建立一個字典（步驟 2 & 3）\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUnUsNPqGIKE"
   },
   "outputs": [],
   "source": [
    "#等到 tokenizer 建好字典以後，我們可以進行上述第 4 個步驟，\n",
    "#請 tokenizer利用內部生成的字典分別將我們的新聞標題 A 與 新聞 B 轉換成數字序列\n",
    "x1_train = tokenizer.texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbns2jSOGIKH",
    "outputId": "10be1a09-c0e7-406e-c527-9337c8d979c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x1_train 為一個 Python list，裡頭包含了每一筆假新聞標題 A 對應的數字序列。\n",
    "x1_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAOxQLwmGIKJ",
    "outputId": "d71ff074-08a1-408f-d667-50a6e87d91c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '的',\n",
       " 2: '了',\n",
       " 3: '是',\n",
       " 4: '你',\n",
       " 5: '被',\n",
       " 6: '谣言',\n",
       " 7: '吃',\n",
       " 8: '辟谣',\n",
       " 9: '这',\n",
       " 10: '不',\n",
       " 11: '人',\n",
       " 12: '有',\n",
       " 13: '吗',\n",
       " 14: '都',\n",
       " 15: '年',\n",
       " 16: '在',\n",
       " 17: '要',\n",
       " 18: '能',\n",
       " 19: '就',\n",
       " 20: '和',\n",
       " 21: '会',\n",
       " 22: '还',\n",
       " 23: '将',\n",
       " 24: '3',\n",
       " 25: '农村',\n",
       " 26: '它',\n",
       " 27: '网友',\n",
       " 28: '岁',\n",
       " 29: '一',\n",
       " 30: '大',\n",
       " 31: '用',\n",
       " 32: '又',\n",
       " 33: '个',\n",
       " 34: '也',\n",
       " 35: '后',\n",
       " 36: '让',\n",
       " 37: '天',\n",
       " 38: '减肥',\n",
       " 39: '5',\n",
       " 40: '多',\n",
       " 41: '月',\n",
       " 42: '我',\n",
       " 43: '这样',\n",
       " 44: '2018',\n",
       " 45: '中国',\n",
       " 46: '与',\n",
       " 47: '来',\n",
       " 48: '上',\n",
       " 49: '看',\n",
       " 50: '知道',\n",
       " 51: '1',\n",
       " 52: '斤',\n",
       " 53: '好',\n",
       " 54: '一个',\n",
       " 55: '可以',\n",
       " 56: '再',\n",
       " 57: '这些',\n",
       " 58: '手机',\n",
       " 59: '农民',\n",
       " 60: '瘦',\n",
       " 61: '为',\n",
       " 62: '喝',\n",
       " 63: '10',\n",
       " 64: '万',\n",
       " 65: '说',\n",
       " 66: '孩子',\n",
       " 67: '新',\n",
       " 68: '他',\n",
       " 69: '曝光',\n",
       " 70: '已',\n",
       " 71: '回应',\n",
       " 72: '到',\n",
       " 73: '这个',\n",
       " 74: '食物',\n",
       " 75: '教',\n",
       " 76: '最',\n",
       " 77: '2',\n",
       " 78: '网传',\n",
       " 79: '不能',\n",
       " 80: '4',\n",
       " 81: '却',\n",
       " 82: '她',\n",
       " 83: '真的',\n",
       " 84: '每天',\n",
       " 85: '补贴',\n",
       " 86: '不用',\n",
       " 87: '种',\n",
       " 88: '怀孕',\n",
       " 89: '中',\n",
       " 90: '离婚',\n",
       " 91: '微信',\n",
       " 92: '做',\n",
       " 93: '7',\n",
       " 94: '没',\n",
       " 95: '方法',\n",
       " 96: '别',\n",
       " 97: '小',\n",
       " 98: '怎么',\n",
       " 99: '白发',\n",
       " 100: '6',\n",
       " 101: '只',\n",
       " 102: '什么',\n",
       " 103: '日',\n",
       " 104: '太',\n",
       " 105: '恋情',\n",
       " 106: '系',\n",
       " 107: '给',\n",
       " 108: '不要',\n",
       " 109: '注意',\n",
       " 110: '传',\n",
       " 111: '结婚',\n",
       " 112: '致癌',\n",
       " 113: '对',\n",
       " 114: '去',\n",
       " 115: '过',\n",
       " 116: '可',\n",
       " 117: '起',\n",
       " 118: '很',\n",
       " 119: '快',\n",
       " 120: '亿',\n",
       " 121: '范冰冰',\n",
       " 122: '轻松',\n",
       " 123: '没有',\n",
       " 124: '快速',\n",
       " 125: '一起',\n",
       " 126: '不是',\n",
       " 127: '终于',\n",
       " 128: '视频',\n",
       " 129: '20',\n",
       " 130: '男子',\n",
       " 131: '8',\n",
       " 132: '一招',\n",
       " 133: '比',\n",
       " 134: '前',\n",
       " 135: '专家',\n",
       " 136: '谢霆锋',\n",
       " 137: '想',\n",
       " 138: '因',\n",
       " 139: '得',\n",
       " 140: '退出',\n",
       " 141: '赵丽颖',\n",
       " 142: '美国',\n",
       " 143: '曝',\n",
       " 144: '加',\n",
       " 145: '发生',\n",
       " 146: '张柏芝',\n",
       " 147: '假',\n",
       " 148: '健康',\n",
       " 149: '可能',\n",
       " 150: '警方',\n",
       " 151: '娱乐圈',\n",
       " 152: '官方',\n",
       " 153: '买',\n",
       " 154: '更',\n",
       " 155: '王菲',\n",
       " 156: '点',\n",
       " 157: '招',\n",
       " 158: '谁',\n",
       " 159: '打',\n",
       " 160: '价值',\n",
       " 161: '造谣',\n",
       " 162: '看看',\n",
       " 163: '花',\n",
       " 164: '宝宝',\n",
       " 165: '15',\n",
       " 166: '儿子',\n",
       " 167: '女人',\n",
       " 168: '称',\n",
       " 169: '女儿',\n",
       " 170: '真相',\n",
       " 171: '事件',\n",
       " 172: '这种',\n",
       " 173: '高',\n",
       " 174: '才',\n",
       " 175: '网警',\n",
       " 176: '元',\n",
       " 177: '到底',\n",
       " 178: '疑似',\n",
       " 179: '取消',\n",
       " 180: '这么',\n",
       " 181: '原来',\n",
       " 182: '杨幂',\n",
       " 183: '竟然',\n",
       " 184: '遭',\n",
       " 185: '皮肤',\n",
       " 186: '原因',\n",
       " 187: '变',\n",
       " 188: '如何',\n",
       " 189: '发现',\n",
       " 190: '越',\n",
       " 191: '把',\n",
       " 192: '钱',\n",
       " 193: '自己',\n",
       " 194: '洗头',\n",
       " 195: '日本',\n",
       " 196: '吧',\n",
       " 197: '有人',\n",
       " 198: '两',\n",
       " 199: '下',\n",
       " 200: '癌症',\n",
       " 201: '里',\n",
       " 202: '出',\n",
       " 203: '最新',\n",
       " 204: '外星人',\n",
       " 205: '30',\n",
       " 206: '多少',\n",
       " 207: '着',\n",
       " 208: '收藏',\n",
       " 209: '睡',\n",
       " 210: '病毒',\n",
       " 211: '生',\n",
       " 212: '朋友圈',\n",
       " 213: '分钟',\n",
       " 214: '时',\n",
       " 215: '治疗',\n",
       " 216: '效果',\n",
       " 217: '2017',\n",
       " 218: '朱之文',\n",
       " 219: '水果',\n",
       " 220: '马云',\n",
       " 221: '排毒',\n",
       " 222: '糖尿病',\n",
       " 223: '成',\n",
       " 224: '开始',\n",
       " 225: '死亡',\n",
       " 226: '简单',\n",
       " 227: '肚子',\n",
       " 228: '不会',\n",
       " 229: '出现',\n",
       " 230: '导致',\n",
       " 231: '即将',\n",
       " 232: '李小璐',\n",
       " 233: '需',\n",
       " 234: '真',\n",
       " 235: '竟',\n",
       " 236: '已经',\n",
       " 237: '三',\n",
       " 238: '我们',\n",
       " 239: '宣布',\n",
       " 240: '变黑',\n",
       " 241: '感染',\n",
       " 242: '但',\n",
       " 243: '分手',\n",
       " 244: '飞机',\n",
       " 245: '苹果',\n",
       " 246: '哭',\n",
       " 247: '从',\n",
       " 248: '治',\n",
       " 249: '身体',\n",
       " 250: '美白',\n",
       " 251: '王思聪',\n",
       " 252: '染发',\n",
       " 253: '突出',\n",
       " 254: '公开',\n",
       " 255: '一定',\n",
       " 256: '老',\n",
       " 257: '全',\n",
       " 258: '千万',\n",
       " 259: '王宝强',\n",
       " 260: '消息',\n",
       " 261: '或',\n",
       " 262: '央视',\n",
       " 263: '功能',\n",
       " 264: '现在',\n",
       " 265: '为什么',\n",
       " 266: '朋友',\n",
       " 267: '第一',\n",
       " 268: '于',\n",
       " 269: '公布',\n",
       " 270: '国家',\n",
       " 271: '曾',\n",
       " 272: '腰间盘',\n",
       " 273: '女',\n",
       " 274: '号',\n",
       " 275: '分',\n",
       " 276: '爆',\n",
       " 277: '生姜',\n",
       " 278: '政策',\n",
       " 279: '为何',\n",
       " 280: '小孩',\n",
       " 281: '女子',\n",
       " 282: '信',\n",
       " 283: '当',\n",
       " 284: '超',\n",
       " 285: '脸',\n",
       " 286: '坚持',\n",
       " 287: '们',\n",
       " 288: '死',\n",
       " 289: '出轨',\n",
       " 290: '网',\n",
       " 291: '男孩',\n",
       " 292: '之',\n",
       " 293: '老公',\n",
       " 294: '赶紧',\n",
       " 295: '李晨',\n",
       " 296: '100',\n",
       " 297: '郑爽',\n",
       " 298: '塑料',\n",
       " 299: '几个',\n",
       " 300: '白',\n",
       " 301: '啦',\n",
       " 302: '华为',\n",
       " 303: '还有',\n",
       " 304: '全国',\n",
       " 305: '就是',\n",
       " 306: '男人',\n",
       " 307: '长',\n",
       " 308: '现场',\n",
       " 309: '60',\n",
       " 310: '如今',\n",
       " 311: '湿气',\n",
       " 312: '中医',\n",
       " 313: '复发',\n",
       " 314: '汽车',\n",
       " 315: '症状',\n",
       " 316: '呢',\n",
       " 317: '发布',\n",
       " 318: '带',\n",
       " 319: 'ufo',\n",
       " 320: '大蒜',\n",
       " 321: '克星',\n",
       " 322: '自制',\n",
       " 323: '粉丝',\n",
       " 324: '大家',\n",
       " 325: '交警',\n",
       " 326: '爱',\n",
       " 327: '战狼',\n",
       " 328: '妈妈',\n",
       " 329: '并',\n",
       " 330: '9',\n",
       " 331: '抗癌',\n",
       " 332: '免费',\n",
       " 333: '瘦身',\n",
       " 334: '支付宝',\n",
       " 335: '整容',\n",
       " 336: '50',\n",
       " 337: '抢',\n",
       " 338: '12',\n",
       " 339: '男',\n",
       " 340: '存在',\n",
       " 341: '请',\n",
       " 342: '便秘',\n",
       " 343: '等',\n",
       " 344: '提醒',\n",
       " 345: '鸡蛋',\n",
       " 346: '呗',\n",
       " 347: '老婆',\n",
       " 348: '告诉',\n",
       " 349: '水',\n",
       " 350: '贾乃亮',\n",
       " 351: '跟',\n",
       " 352: '传闻',\n",
       " 353: '一次',\n",
       " 354: '出来',\n",
       " 355: '美',\n",
       " 356: '事',\n",
       " 357: '严重',\n",
       " 358: '新规',\n",
       " 359: '黑发',\n",
       " 360: '告别',\n",
       " 361: '医生',\n",
       " 362: '女友',\n",
       " 363: '腾讯',\n",
       " 364: '哪些',\n",
       " 365: '胡歌',\n",
       " 366: '文华',\n",
       " 367: '关于',\n",
       " 368: '微',\n",
       " 369: '三天',\n",
       " 370: '东西',\n",
       " 371: '今年',\n",
       " 372: '拍',\n",
       " 373: '刘恺威',\n",
       " 374: '正式',\n",
       " 375: '一把',\n",
       " 376: '很多',\n",
       " 377: '夏天',\n",
       " 378: '纯属',\n",
       " 379: '医院',\n",
       " 380: '爆料',\n",
       " 381: '未',\n",
       " 382: '谢娜',\n",
       " 383: '开',\n",
       " 384: '张翰',\n",
       " 385: '二胎',\n",
       " 386: '发',\n",
       " 387: '拿',\n",
       " 388: '再也',\n",
       " 389: '博',\n",
       " 390: '澄清',\n",
       " 391: '秒',\n",
       " 392: '多年',\n",
       " 393: '白蛾',\n",
       " 394: '套现',\n",
       " 395: '18',\n",
       " 396: '开拍',\n",
       " 397: '靠',\n",
       " 398: '家里',\n",
       " 399: '怎么办',\n",
       " 400: '还是',\n",
       " 401: '洋葱',\n",
       " 402: '罚款',\n",
       " 403: '恢复',\n",
       " 404: '偏方',\n",
       " 405: '鹿晗',\n",
       " 406: '吴京',\n",
       " 407: '情况',\n",
       " 408: '不到',\n",
       " 409: '车',\n",
       " 410: '黑',\n",
       " 411: '刘涛',\n",
       " 412: '者',\n",
       " 413: '几种',\n",
       " 414: '成功',\n",
       " 415: '高血压',\n",
       " 416: '偷',\n",
       " 417: '不再',\n",
       " 418: '父母',\n",
       " 419: '最后',\n",
       " 420: '地震',\n",
       " 421: '聊天记录',\n",
       " 422: '容易',\n",
       " 423: '地球',\n",
       " 424: '常',\n",
       " 425: '完',\n",
       " 426: '泡',\n",
       " 427: '居然',\n",
       " 428: '信息',\n",
       " 429: '最高',\n",
       " 430: '现身',\n",
       " 431: '影响',\n",
       " 432: '再次',\n",
       " 433: '承认',\n",
       " 434: '直接',\n",
       " 435: '赚钱',\n",
       " 436: '马蓉',\n",
       " 437: '一周',\n",
       " 438: '小米',\n",
       " 439: '见效',\n",
       " 440: '反弹',\n",
       " 441: '明年',\n",
       " 442: '竟是',\n",
       " 443: '中毒',\n",
       " 444: '复合',\n",
       " 445: '血管',\n",
       " 446: '真实',\n",
       " 447: '价格',\n",
       " 448: '如',\n",
       " 449: '祛斑',\n",
       " 450: '刮油',\n",
       " 451: 'wifi',\n",
       " 452: '唱',\n",
       " 453: '万元',\n",
       " 454: '等于',\n",
       " 455: '消失',\n",
       " 456: '怀',\n",
       " 457: '问题',\n",
       " 458: '男友',\n",
       " 459: '血糖',\n",
       " 460: '字',\n",
       " 461: '听',\n",
       " 462: '孕妇',\n",
       " 463: '王者',\n",
       " 464: '未来',\n",
       " 465: '高考',\n",
       " 466: '荣耀',\n",
       " 467: '一点',\n",
       " 468: '时间',\n",
       " 469: '演员',\n",
       " 470: '关系',\n",
       " 471: '放',\n",
       " 472: '那',\n",
       " 473: '北京',\n",
       " 474: '帮',\n",
       " 475: '头发',\n",
       " 476: '世界',\n",
       " 477: '走',\n",
       " 478: '啊',\n",
       " 479: '穿',\n",
       " 480: '司机',\n",
       " 481: '其实',\n",
       " 482: '内',\n",
       " 483: '宅基地',\n",
       " 484: '晚上',\n",
       " 485: '唐嫣',\n",
       " 486: '照片',\n",
       " 487: '酒',\n",
       " 488: '因为',\n",
       " 489: '老人',\n",
       " 490: '标准',\n",
       " 491: '孕期',\n",
       " 492: '两个',\n",
       " 493: '撒药治',\n",
       " 494: '晒',\n",
       " 495: '市场',\n",
       " 496: '地',\n",
       " 497: '经常',\n",
       " 498: '充电',\n",
       " 499: '假消息',\n",
       " 500: '生活',\n",
       " 501: '表示',\n",
       " 502: '行业',\n",
       " 503: '解决',\n",
       " 504: '提现',\n",
       " 505: '发文',\n",
       " 506: '以下',\n",
       " 507: '其',\n",
       " 508: '散布',\n",
       " 509: '安全',\n",
       " 510: '美容',\n",
       " 511: '揭秘',\n",
       " 512: '明星',\n",
       " 513: '信号',\n",
       " 514: '需要',\n",
       " 515: '豪宅',\n",
       " 516: '小伙',\n",
       " 517: '跑',\n",
       " 518: '今天',\n",
       " 519: '该',\n",
       " 520: '丨',\n",
       " 521: '首次',\n",
       " 522: '跳楼',\n",
       " 523: '西瓜',\n",
       " 524: '百万',\n",
       " 525: '开车',\n",
       " 526: '李易峰',\n",
       " 527: '去世',\n",
       " 528: '结果',\n",
       " 529: '找',\n",
       " 530: '领',\n",
       " 531: '妻子',\n",
       " 532: '公司',\n",
       " 533: 'iphone',\n",
       " 534: '一下',\n",
       " 535: '龙头',\n",
       " 536: '独生子女',\n",
       " 537: '卖',\n",
       " 538: '硬币',\n",
       " 539: '狗',\n",
       " 540: '只有',\n",
       " 541: '拘留',\n",
       " 542: '工作室',\n",
       " 543: '娜扎',\n",
       " 544: '小妙',\n",
       " 545: '危害',\n",
       " 546: '如果',\n",
       " 547: '离开',\n",
       " 548: '涨',\n",
       " 549: '无',\n",
       " 550: '驾驶证',\n",
       " 551: '有效',\n",
       " 552: '关闭',\n",
       " 553: '25',\n",
       " 554: '补偿',\n",
       " 555: '骗',\n",
       " 556: '照',\n",
       " 557: '驾',\n",
       " 558: '赚',\n",
       " 559: '40',\n",
       " 560: '好消息',\n",
       " 561: '大肚腩',\n",
       " 562: '刘德华',\n",
       " 563: '11',\n",
       " 564: '全面',\n",
       " 565: '倍',\n",
       " 566: '拒绝',\n",
       " 567: '娶',\n",
       " 568: '密码',\n",
       " 569: '卓伟',\n",
       " 570: '试试',\n",
       " 571: '你们',\n",
       " 572: '秘密',\n",
       " 573: '抓',\n",
       " 574: '连',\n",
       " 575: '准备',\n",
       " 576: '只要',\n",
       " 577: '像',\n",
       " 578: '吸毒',\n",
       " 579: '首歌',\n",
       " 580: '甲醛',\n",
       " 581: '京东',\n",
       " 582: '见',\n",
       " 583: '养老金',\n",
       " 584: '一杯',\n",
       " 585: '蔬菜',\n",
       " 586: '白醋',\n",
       " 587: '茶',\n",
       " 588: '吴昕',\n",
       " 589: '是否',\n",
       " 590: '生男生女',\n",
       " 591: '起来',\n",
       " 592: '牙膏',\n",
       " 593: '儿童',\n",
       " 594: '千万别',\n",
       " 595: '牛',\n",
       " 596: '喝酒',\n",
       " 597: '一个月',\n",
       " 598: '神奇',\n",
       " 599: '而',\n",
       " 600: '回归',\n",
       " 601: '林心如',\n",
       " 602: '掉',\n",
       " 603: '三个',\n",
       " 604: '规定',\n",
       " 605: '图',\n",
       " 606: '婚礼',\n",
       " 607: '喝一杯',\n",
       " 608: '暴涨',\n",
       " 609: '眼袋',\n",
       " 610: '收购',\n",
       " 611: '香蕉',\n",
       " 612: '人民币',\n",
       " 613: '出台',\n",
       " 614: '出狱',\n",
       " 615: '景甜',\n",
       " 616: '女孩',\n",
       " 617: '这里',\n",
       " 618: '机场',\n",
       " 619: '年轻',\n",
       " 620: '喜欢',\n",
       " 621: '这次',\n",
       " 622: '那么',\n",
       " 623: '上海',\n",
       " 624: '一张',\n",
       " 625: '为了',\n",
       " 626: '成龙',\n",
       " 627: '次',\n",
       " 628: '成为',\n",
       " 629: '全部',\n",
       " 630: '神秘',\n",
       " 631: '白头发',\n",
       " 632: '90',\n",
       " 633: '王',\n",
       " 634: '话',\n",
       " 635: '担心',\n",
       " 636: '怕',\n",
       " 637: '管用',\n",
       " 638: '紧急',\n",
       " 639: '难',\n",
       " 640: '网上',\n",
       " 641: '强',\n",
       " 642: '同',\n",
       " 643: '厉害',\n",
       " 644: '身亡',\n",
       " 645: '工资',\n",
       " 646: '一种',\n",
       " 647: '项',\n",
       " 648: '成都',\n",
       " 649: '以后',\n",
       " 650: '宋',\n",
       " 651: '最大',\n",
       " 652: '封杀',\n",
       " 653: '分享',\n",
       " 654: '看到',\n",
       " 655: '袭',\n",
       " 656: '敢',\n",
       " 657: '自杀',\n",
       " 658: '城市',\n",
       " 659: '体重',\n",
       " 660: '不敢',\n",
       " 661: '你家',\n",
       " 662: '之后',\n",
       " 663: '相信',\n",
       " 664: '一天',\n",
       " 665: '最好',\n",
       " 666: '加盟',\n",
       " 667: '近',\n",
       " 668: '春晚',\n",
       " 669: '腰',\n",
       " 670: '致',\n",
       " 671: '变化',\n",
       " 672: '养颜',\n",
       " 673: '删除',\n",
       " 674: '传言',\n",
       " 675: '打开',\n",
       " 676: '小时',\n",
       " 677: '使用',\n",
       " 678: '深圳',\n",
       " 679: '深夜',\n",
       " 680: '斑点',\n",
       " 681: '类',\n",
       " 682: '学会',\n",
       " 683: '家长',\n",
       " 684: '张一山',\n",
       " 685: '辐射',\n",
       " 686: '降',\n",
       " 687: '一年',\n",
       " 688: '没想到',\n",
       " 689: '向',\n",
       " 690: '彻底',\n",
       " 691: '蚊子',\n",
       " 692: '李',\n",
       " 693: '刘诗诗',\n",
       " 694: '送',\n",
       " 695: '人类',\n",
       " 696: '破',\n",
       " 697: '体内',\n",
       " 698: '车主',\n",
       " 699: '表白',\n",
       " 700: '新农',\n",
       " 701: '突然',\n",
       " 702: '领取',\n",
       " 703: '那英',\n",
       " 704: '疯',\n",
       " 705: '仅',\n",
       " 706: '项目',\n",
       " 707: 'sk5',\n",
       " 708: '房屋',\n",
       " 709: '纸币',\n",
       " 710: '婚姻',\n",
       " 711: '怒',\n",
       " 712: 'ofo',\n",
       " 713: '关晓彤',\n",
       " 714: '证实',\n",
       " 715: '推荐',\n",
       " 716: '年底',\n",
       " 717: '营养',\n",
       " 718: '脸上',\n",
       " 719: '哥',\n",
       " 720: '赵本山',\n",
       " 721: '么',\n",
       " 722: '网络',\n",
       " 723: '小心',\n",
       " 724: '难道',\n",
       " 725: '胎儿',\n",
       " 726: '至',\n",
       " 727: '退休',\n",
       " 728: '学',\n",
       " 729: '参加',\n",
       " 730: '干净',\n",
       " 731: '马化腾',\n",
       " 732: '实施',\n",
       " 733: '提前',\n",
       " 734: '洗脸',\n",
       " 735: '上市',\n",
       " 736: '转发',\n",
       " 737: '学生',\n",
       " 738: '吴奇隆',\n",
       " 739: '实用',\n",
       " 740: '无法',\n",
       " 741: '1000',\n",
       " 742: '仍',\n",
       " 743: '白酒',\n",
       " 744: '时候',\n",
       " 745: '食品',\n",
       " 746: '人民',\n",
       " 747: '擦',\n",
       " 748: '紫菜',\n",
       " 749: '换',\n",
       " 750: '实',\n",
       " 751: '怎样',\n",
       " 752: '住',\n",
       " 753: '爸爸',\n",
       " 754: '透露',\n",
       " 755: '垃圾',\n",
       " 756: '天然',\n",
       " 757: '真正',\n",
       " 758: '家人',\n",
       " 759: '错',\n",
       " 760: '毒',\n",
       " 761: '背后',\n",
       " 762: '女生',\n",
       " 763: '早',\n",
       " 764: '慢性',\n",
       " 765: '心疼',\n",
       " 766: '合',\n",
       " 767: '他们',\n",
       " 768: '相克',\n",
       " 769: '刘强东',\n",
       " 770: '传播',\n",
       " 771: '疑',\n",
       " 772: '秘方',\n",
       " 773: '十大',\n",
       " 774: '少',\n",
       " 775: '患者',\n",
       " 776: '根治',\n",
       " 777: '去除',\n",
       " 778: '照曝光',\n",
       " 779: '所有',\n",
       " 780: '找到',\n",
       " 781: '瞬间',\n",
       " 782: '美女',\n",
       " 783: '重大',\n",
       " 784: '预防',\n",
       " 785: '意外',\n",
       " 786: '只是',\n",
       " 787: '花钱',\n",
       " 788: '马上',\n",
       " 789: '肉',\n",
       " 790: '花生',\n",
       " 791: '搞定',\n",
       " 792: '驾照',\n",
       " 793: '诞生',\n",
       " 794: '手',\n",
       " 795: '家',\n",
       " 796: '了解',\n",
       " 797: '严查',\n",
       " 798: '收费',\n",
       " 799: '胖',\n",
       " 800: '张继科',\n",
       " 801: '配',\n",
       " 802: '不想',\n",
       " 803: '获',\n",
       " 804: '科学',\n",
       " 805: '真是',\n",
       " 806: '养',\n",
       " 807: '17',\n",
       " 808: '李亚鹏',\n",
       " 809: '痛哭',\n",
       " 810: '热巴',\n",
       " 811: '前妻',\n",
       " 812: '大妈',\n",
       " 813: '土地',\n",
       " 814: '植物',\n",
       " 815: '数字',\n",
       " 816: '传谣',\n",
       " 817: '不可',\n",
       " 818: '升级',\n",
       " 819: 'baby',\n",
       " 820: '可乐',\n",
       " 821: '人员',\n",
       " 822: '市',\n",
       " 823: '一样',\n",
       " 824: '脂肪',\n",
       " 825: '药',\n",
       " 826: '骂',\n",
       " 827: '谣',\n",
       " 828: '以上',\n",
       " 829: '杨',\n",
       " 830: '上涨',\n",
       " 831: '省',\n",
       " 832: '建华',\n",
       " 833: '不好',\n",
       " 834: '躺',\n",
       " 835: '版',\n",
       " 836: '接',\n",
       " 837: '感冒',\n",
       " 838: '自',\n",
       " 839: '听说',\n",
       " 840: '一生',\n",
       " 841: '世界杯',\n",
       " 842: '霍',\n",
       " 843: '一首歌',\n",
       " 844: '不仅',\n",
       " 845: '调整',\n",
       " 846: '要求',\n",
       " 847: '暴瘦',\n",
       " 848: '刚刚',\n",
       " 849: '洗',\n",
       " 850: '减到',\n",
       " 851: '元宝',\n",
       " 852: '14',\n",
       " 853: '加点',\n",
       " 854: '大衣',\n",
       " 855: '后悔',\n",
       " 856: '选择',\n",
       " 857: '潘玮柏',\n",
       " 858: '网民',\n",
       " 859: '光绪',\n",
       " 860: '技巧',\n",
       " 861: '死对头',\n",
       " 862: '疼',\n",
       " 863: '林志玲',\n",
       " 864: '张杰',\n",
       " 865: '动作',\n",
       " 866: '回',\n",
       " 867: '是不是',\n",
       " 868: '倒闭',\n",
       " 869: '病',\n",
       " 870: '运动',\n",
       " 871: '吴亦凡',\n",
       " 872: '建房',\n",
       " 873: '常见',\n",
       " 874: '股票',\n",
       " 875: '鼻炎',\n",
       " 876: '霸气',\n",
       " 877: '泡水',\n",
       " 878: '企业',\n",
       " 879: '那些',\n",
       " 880: '节目',\n",
       " 881: '否认',\n",
       " 882: '13',\n",
       " 883: '红',\n",
       " 884: '醋',\n",
       " 885: '老中医',\n",
       " 886: '患',\n",
       " 887: '家庭',\n",
       " 888: '刀郎',\n",
       " 889: '有望',\n",
       " 890: '拘',\n",
       " 891: '旧',\n",
       " 892: '断根',\n",
       " 893: '刘亦菲',\n",
       " 894: '杨钰莹',\n",
       " 895: '地方',\n",
       " 896: '防癌',\n",
       " 897: '挺',\n",
       " 898: '不管',\n",
       " 899: '观众',\n",
       " 900: '节食',\n",
       " 901: '丈夫',\n",
       " 902: '染',\n",
       " 903: '片酬',\n",
       " 904: '无人',\n",
       " 905: '近照',\n",
       " 906: '期间',\n",
       " 907: '希望',\n",
       " 908: '养生',\n",
       " 909: '民间',\n",
       " 910: '定',\n",
       " 911: '越来越',\n",
       " 912: '花椒',\n",
       " 913: '规',\n",
       " 914: '在家',\n",
       " 915: '房价',\n",
       " 916: '决定',\n",
       " 917: '恒大',\n",
       " 918: '枸杞',\n",
       " 919: '罗晋',\n",
       " 920: '应该',\n",
       " 921: '驾考',\n",
       " 922: '科学家',\n",
       " 923: '出生',\n",
       " 924: '输入',\n",
       " 925: '爆炸',\n",
       " 926: '现',\n",
       " 927: '肾',\n",
       " 928: '19',\n",
       " 929: '血压',\n",
       " 930: '全球',\n",
       " 931: '年终奖',\n",
       " 932: '重',\n",
       " 933: '员工',\n",
       " 934: '提高',\n",
       " 935: '腰腿疼',\n",
       " 936: '某',\n",
       " 937: '万达',\n",
       " 938: '准',\n",
       " 939: '按',\n",
       " 940: '哪',\n",
       " 941: '必须',\n",
       " 942: '丢',\n",
       " 943: '怼',\n",
       " 944: '拆迁',\n",
       " 945: '最佳',\n",
       " 946: '谢贤',\n",
       " 947: '媒体',\n",
       " 948: '打死',\n",
       " 949: '宝',\n",
       " 950: '声明',\n",
       " 951: '农民工',\n",
       " 952: '姚明',\n",
       " 953: '实行',\n",
       " 954: '尴尬',\n",
       " 955: '啥',\n",
       " 956: '差',\n",
       " 957: '确定',\n",
       " 958: '刚',\n",
       " 959: '隐藏',\n",
       " 960: '限购',\n",
       " 961: '赵薇',\n",
       " 962: '牙齿',\n",
       " 963: '国产',\n",
       " 964: '一枚',\n",
       " 965: '戒烟',\n",
       " 966: '妙',\n",
       " 967: '团队',\n",
       " 968: '降血压',\n",
       " 969: '还要',\n",
       " 970: '母亲',\n",
       " 971: '交',\n",
       " 972: '外卖',\n",
       " 973: '可怕',\n",
       " 974: '究竟',\n",
       " 975: 'h7n9',\n",
       " 976: '关注',\n",
       " 977: '一直',\n",
       " 978: '热',\n",
       " 979: '煮',\n",
       " 980: '看完',\n",
       " 981: '广州',\n",
       " 982: '房子',\n",
       " 983: '暴跌',\n",
       " 984: '白条',\n",
       " 985: '名',\n",
       " 986: '一句',\n",
       " 987: '根',\n",
       " 988: '身价',\n",
       " 989: '芯片',\n",
       " 990: '姐',\n",
       " 991: '啤酒',\n",
       " 992: '是因为',\n",
       " 993: '女性',\n",
       " 994: '楼市',\n",
       " 995: '印',\n",
       " 996: '七天',\n",
       " 997: '四个',\n",
       " 998: '特征',\n",
       " 999: '祝福',\n",
       " 1000: '200',\n",
       " ...}"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcrTxh5nGIKM",
    "outputId": "b5275f31-315c-47c1-a6a5-a5d79af4e6ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "#我們也可以利用 tokenizer.index_word 來將索引數字對應回本來的詞彙\n",
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJdvS_JuGIKO",
    "outputId": "2d9a5cc1-2ead-4391-e9bc-eaa95106ba54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "19\n",
      "19\n",
      "19\n",
      "9\n",
      "19\n",
      "6\n",
      "19\n",
      "14\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#發現每個序列長度都不一樣\n",
    "for seq in x1_train[:10]:\n",
    "    print(len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw-_fQW3GIKQ",
    "outputId": "f0f88a85-ff6b-4c2b-9a90-22f01423b219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最長的序列有61個詞彙\n",
    "max_seq_len = max([len(seq) for seq in x1_train])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_8MANMAGIKT"
   },
   "outputs": [],
   "source": [
    "#將每一個序列長度調整成一樣\n",
    "#不夠的前面補0\n",
    "#超過的尾巴刪掉\n",
    "#一般來說 MAX_SEQUENCE_LENGTH 可以設定成最長序列的長度（此例中的 61）\n",
    "#但這邊為了讓模型可以只看前 20 個詞彙就做出判斷以節省訓練時間\n",
    "#我們先暫時使用 20 這個數字。\n",
    "#using package pad_sequence\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras.preprocessing.sequence.pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_train = keras.preprocessing.sequence.pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WKCyZqHGIKV",
    "outputId": "c492e804-7d06-40ba-fbb9-e7ad33fbdf97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1850,    2,   13],\n",
       "       [   0,    4,   10, ...,   23,  284, 1181],\n",
       "       [   0,    4,   10, ...,   23,  284, 1181],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 3955, 5467,   30],\n",
       "       [   0,    0,    0, ..., 3955, 5467,   30],\n",
       "       [   0,    0,    0, ...,   21,  113,   13]])"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sbqlu8wjGIKX",
    "outputId": "998f3cbc-87bb-4f8d-b1e6-fbebaa9a542f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有新聞標題的序列長度皆為 20 !\n"
     ]
    }
   ],
   "source": [
    "#assert斷言，不滿足條件時，觸發異常\n",
    "#all sequence length are 20\n",
    "for seq in x1_train + x2_train:\n",
    "    assert len(seq) == 20\n",
    "    \n",
    "print(\"所有新聞標題的序列長度皆為 20 !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cRhbu7XGIKb",
    "outputId": "57c41c54-52c4-4a9a-9171-2104156818c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0         unrelated\n",
       "3         unrelated\n",
       "1         unrelated\n",
       "2         unrelated\n",
       "9            agreed\n",
       "            ...    \n",
       "321185    unrelated\n",
       "321182    unrelated\n",
       "321184    unrelated\n",
       "321186    unrelated\n",
       "321188    unrelated\n",
       "Name: label, Length: 320552, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6sFYwI6GIKd"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "#製造一個字典，定義每一個分類對應到的索引數字\n",
    "label_to_index = {\n",
    "    'unrelated': 0, \n",
    "    'agreed': 1, \n",
    "    'disagreed': 2\n",
    "}\n",
    "\n",
    "# 將分類標籤對應到剛定義的數字\n",
    "y_train = train.label.apply(lambda x: label_to_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTLwkcKiGIKf",
    "outputId": "742bbed4-3918-4864-b9c7-33396d7a4eb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0         0\n",
       "3         0\n",
       "1         0\n",
       "2         0\n",
       "9         1\n",
       "         ..\n",
       "321185    0\n",
       "321182    0\n",
       "321184    0\n",
       "321186    0\n",
       "321188    0\n",
       "Name: label, Length: 320552, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Dzm7M66GIKi",
    "outputId": "babdea87-d84d-4d3d-e227-a001014054a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLsFUZGIGIKk",
    "outputId": "9263f4bd-5e6c-4c38-bbd9-4d4eca7359c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#現在每個分類的文字標籤都已經被轉成對應的數字，\n",
    "#接著讓我們利用 Keras 做 One Hot Encoding\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGBuWOMmGIKn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "# 小彩蛋\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        x1_train, x2_train, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qy1Mo5EyGIKp",
    "outputId": "49498c30-e28e-4734-f420-26e1099cf652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (288496, 20)\n",
      "x2_train: (288496, 20)\n",
      "y_train : (288496, 3)\n",
      "----------\n",
      "x1_val:   (32056, 20)\n",
      "x2_val:   (32056, 20)\n",
      "y_val :   (32056, 3)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGZCWtlQGIKr",
    "outputId": "7e50f983-5810-4088-ff35-5b301a509da9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  array([   0,    0,    0,    0,    0, 4178, 2972,    9,   80,   87,  717,\n",
       "           18,  474,    4,  968,    4,  823,   14, 1436,  721])),\n",
       " (1,\n",
       "  array([   0,    0,    0,    0,    0,    0,    0,    0,    0,  411,  308,\n",
       "          809, 3142,   17,   90,  434,  191, 3713,    1,    2])),\n",
       " (2,\n",
       "  array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         5212,    5, 8867,  793, 1063, 3626,  642,  337, 1172])),\n",
       " (3,\n",
       "  array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0, 2832,  244,  493,  393])),\n",
       " (4,\n",
       "  array([   0,    0,    0,    0,    0,    0,    0,    0,  290,  143, 2523,\n",
       "         2380,   46, 3120,   70,  243,  178, 2238, 3945, 1082]))]"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(x1_train[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZ8fVkU8GIKv",
    "outputId": "4631fca6-147c-4ed8-9b59-a2d4dce87d30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新聞標題 1: \n",
      "[   0    0    0    0    0 4178 2972    9   80   87  717   18  474    4\n",
      "  968    4  823   14 1436  721]\n",
      "\n",
      "新聞標題 2: \n",
      "[   0    0    0    0    0    0    0    0    0  411  308  809 3142   17\n",
      "   90  434  191 3713    1    2]\n",
      "\n",
      "新聞標題 3: \n",
      "[   0    0    0    0    0    0    0    0    0    0    0 5212    5 8867\n",
      "  793 1063 3626  642  337 1172]\n",
      "\n",
      "新聞標題 4: \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 2832  244  493  393]\n",
      "\n",
      "新聞標題 5: \n",
      "[   0    0    0    0    0    0    0    0  290  143 2523 2380   46 3120\n",
      "   70  243  178 2238 3945 1082]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(x1_train[:5]):\n",
    "    print(f\"新聞標題 {i + 1}: \")\n",
    "    print(seq)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K2ho6C5GIKx",
    "outputId": "3437bae2-90f9-47d2-ccb8-37b896cde092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新聞標題 1: \n",
      "['', '', '', '', '', '营养师', '补充', '这', '4', '种', '营养', '能', '帮', '你', '降血压', '你', '一样', '都', '不吃', '么']\n",
      "\n",
      "新聞標題 2: \n",
      "['', '', '', '', '', '', '', '', '', '刘涛', '现场', '痛哭', '发飙', '要', '离婚', '直接', '把', '旁边', '的', '了']\n",
      "\n",
      "新聞標題 3: \n",
      "['', '', '', '', '', '', '', '', '', '', '', 'nba', '被', '球星', '诞生', '火箭', '骑士', '同', '抢', '交易']\n",
      "\n",
      "新聞標題 4: \n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '海口', '飞机', '撒药治', '白蛾']\n",
      "\n",
      "新聞標題 5: \n",
      "['', '', '', '', '', '', '', '', '网', '曝', '杜', '海涛', '与', '沈梦辰', '已', '分手', '疑似', '女方', '劈', '腿']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#用 tokenizer裡頭的字典 index_word 還原文本\n",
    "#get(idx, '') -> 0 -> ''\n",
    "for i, seq in enumerate(x1_train[:5]):\n",
    "    print(f\"新聞標題 {i + 1}: \")\n",
    "    print([tokenizer.index_word.get(idx, '') for idx in seq])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lO4AqKZ1GIKz"
   },
   "outputs": [],
   "source": [
    "#model setting\n",
    "# 基本參數設置，有幾個分類\n",
    "NUM_CLASSES = 3\n",
    "# 在語料庫裡有多少詞彙\n",
    "MAX_NUM_WORDS = 10000\n",
    "# 一個標題最長有幾個詞彙\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "# 一個詞向量的維度\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "# LSTM 輸出的向量維度\n",
    "NUM_LSTM_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HV5-aNXGIK1"
   },
   "outputs": [],
   "source": [
    "# 建立孿生 LSTM 架構（Siamese LSTM）\n",
    "from keras import Input\n",
    "from keras.layers import Embedding, \\\n",
    "    LSTM, concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# 分別定義 2 個新聞標題 A & B 為模型輸入\n",
    "# 兩個標題都是一個長度為 20 的數字序列\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "# 詞嵌入層\n",
    "# 經過詞嵌入層的轉換，兩個新聞標題都變成\n",
    "# 一個詞向量的序列，而每個詞向量的維度\n",
    "# 為 256\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "# LSTM 層\n",
    "# 兩個新聞標題經過此層後\n",
    "# 為一個 128 維度向量\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "# 串接層將兩個新聞標題的結果串接單一向量\n",
    "# 方便跟全連結層相連\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "# 全連接層搭配 Softmax Activation\n",
    "# 可以回傳 3 個成對標題\n",
    "# 屬於各類別的可能機率\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "# 我們的模型就是將數字序列的輸入，轉換\n",
    "# 成 3 個分類的機率的所有步驟 / 層的總和\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuP8RM2YGIK3",
    "outputId": "848dd9d9-a759-4eb6-be3d-cc76223c8c31",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 256)      2560000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          197120      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            771         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,757,891\n",
      "Trainable params: 2,757,891\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGOIsqP0GIK5"
   },
   "outputs": [],
   "source": [
    "#def model's loss function -> cross-entropy\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jz094oOqGIK8",
    "outputId": "686fbe50-94a3-44b4-ba39-37e6d1075e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 288496 samples, validate on 32056 samples\n",
      "Epoch 1/100\n",
      "288496/288496 [==============================] - 34s 117us/step - loss: 0.0907 - accuracy: 0.9675 - val_loss: 0.5736 - val_accuracy: 0.8490\n",
      "Epoch 2/100\n",
      "288496/288496 [==============================] - 34s 117us/step - loss: 0.0880 - accuracy: 0.9691 - val_loss: 0.6008 - val_accuracy: 0.8530\n",
      "Epoch 3/100\n",
      "288496/288496 [==============================] - 34s 116us/step - loss: 0.0866 - accuracy: 0.9692 - val_loss: 0.5644 - val_accuracy: 0.8491\n",
      "Epoch 4/100\n",
      "288496/288496 [==============================] - 34s 117us/step - loss: 0.0855 - accuracy: 0.9696 - val_loss: 0.5916 - val_accuracy: 0.8509\n",
      "Epoch 5/100\n",
      "288496/288496 [==============================] - 34s 117us/step - loss: 0.0841 - accuracy: 0.9701 - val_loss: 0.6211 - val_accuracy: 0.8461\n",
      "Epoch 6/100\n",
      "288496/288496 [==============================] - 34s 116us/step - loss: 0.0828 - accuracy: 0.9705 - val_loss: 0.6054 - val_accuracy: 0.8474\n",
      "Epoch 7/100\n",
      "288496/288496 [==============================] - 34s 117us/step - loss: 0.0820 - accuracy: 0.9709 - val_loss: 0.6093 - val_accuracy: 0.8465\n",
      "Epoch 8/100\n",
      "288496/288496 [==============================] - 34s 116us/step - loss: 0.0814 - accuracy: 0.9710 - val_loss: 0.6209 - val_accuracy: 0.8503\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 決定一次要放多少成對標題給模型訓練\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# 決定模型要看整個訓練資料集幾遍\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "#early stopping\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#vol acc -> mode = max\n",
    "#loss -> mode = min\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode=\"min\")\n",
    "\n",
    "# 實際訓練模型\n",
    "history = model.fit(\n",
    "    # 輸入是兩個長度為 20 的數字序列\n",
    "    x=[x1_train, x2_train], \n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    # 每個 epoch 完後計算驗證資料集\n",
    "    # 上的 Loss 以及準確度\n",
    "    validation_data=(\n",
    "        [x1_val, x2_val], \n",
    "        y_val\n",
    "    ),\n",
    "    # 每個 epoch 隨機調整訓練資料集\n",
    "    # 裡頭的數據以讓訓練過程更穩定\n",
    "    callbacks=[callback],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxgE8iHEGIK-",
    "outputId": "cf7df179-f395-4070-b5cd-edc190c22d0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321187</th>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321190</th>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321189</th>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tid1    tid2                        title1_zh  \\\n",
       "id                                                        \n",
       "321187  167562   59521  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321190  167564   91315              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321189  167563  167564    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗   \n",
       "\n",
       "                          title2_zh  \\\n",
       "id                                    \n",
       "321187  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "321190    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "321189          萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "\n",
       "                                                title1_en  \\\n",
       "id                                                          \n",
       "321187  egypt 's presidential election failed to win m...   \n",
       "321190  A message from Saddam Hussein after he was cap...   \n",
       "321189  Will the United States wage war on Iraq withou...   \n",
       "\n",
       "                                                title2_en  \n",
       "id                                                         \n",
       "321187  Lyon! Lyon officials have denied that Felipe F...  \n",
       "321190  The Top 10 Americans believe that the Lizard M...  \n",
       "321189  A message from Saddam Hussein after he was cap...  "
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('./data/test.csv', index_col=0)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcgGcIhoGILA"
   },
   "outputs": [],
   "source": [
    "# 以下步驟分別對新聞標題 A、B　進行\n",
    "# 文本斷詞 / Word Segmentation\n",
    "t3 = pd.DataFrame(test['title1_zh'].astype(str))\n",
    "t4 = pd.DataFrame(test['title2_zh'].astype(str))\n",
    "test['title1_tokenized'] = t3['title1_zh'].apply(jieba_tokenizer)\n",
    "test['title2_tokenized'] = t4['title2_zh'].apply(jieba_tokenizer)\n",
    "\n",
    "# 將詞彙序列轉為索引數字的序列\n",
    "x1_test = tokenizer.texts_to_sequences(test.title1_tokenized)\n",
    "x2_test = tokenizer.texts_to_sequences(test.title2_tokenized)\n",
    "\n",
    "# 為數字序列加入 zero padding\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(\n",
    "        x1_test, \n",
    "        maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(\n",
    "        x2_test, \n",
    "        maxlen=MAX_SEQUENCE_LENGTH)    \n",
    "\n",
    "# 利用已訓練的模型做預測\n",
    "predictions = model.predict([x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6L1sUK9GILC",
    "outputId": "7238a975-1613-4f9a-f271-c31ff37889e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.3350196e-01, 1.3991307e-07, 6.6497929e-02],\n",
       "       [9.9777251e-01, 5.6404252e-07, 2.2269208e-03],\n",
       "       [9.5004278e-01, 4.9955778e-02, 1.5173633e-06],\n",
       "       [9.9974674e-01, 4.0662785e-06, 2.4917306e-04],\n",
       "       [9.7705609e-01, 6.1702972e-06, 2.2937717e-02]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClhVtzQfGILE",
    "outputId": "e46101c2-4ed2-4bea-c92f-3c139cf59618"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321193</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321191</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   Category\n",
       "0  321187  unrelated\n",
       "1  321190  unrelated\n",
       "2  321189  unrelated\n",
       "3  321193  unrelated\n",
       "4  321191  unrelated"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "#argmax -> 回傳prediction(3個機率)中，最大的索引值，0/1/2\n",
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
    "\n",
    "submission = test.loc[:, ['Category']].reset_index()\n",
    "\n",
    "submission.columns = ['Id', 'Category']\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOaVvxkPGILH",
    "outputId": "9634e817-076d-4739-edd8-2adb9115ebaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'unrelated', 1: 'agreed', 2: 'disagreed'}"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_T8T4P4RGILK",
    "outputId": "9e33d511-7450-42eb-ca8e-8577724ecd87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('unrelated', 0), ('agreed', 1), ('disagreed', 2)])"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSY89oNgGILN",
    "outputId": "9e5c6dec-401c-45a1-9d99-d07cc1c6af13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3g2Sn2QNGILQ"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FdH50uUFGILR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ipywidgets import IntProgress\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBpJMRkVGILT",
    "outputId": "9ff19aab-cc12-42da-838a-0e08651b0354",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.5.0\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzX3258jGILV",
    "outputId": "e7ecd88c-b0a7-49e6-a239-64cacef4f1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHaTRfFNGILX",
    "outputId": "962932af-6c17-488b-83cd-94c2420fa08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token               index          \n",
      "-------------------------\n",
      "##孛                 15160\n",
      "糊                    5128\n",
      "##冾                 14168\n",
      "矛                    4757\n",
      "髂                    7762\n",
      "##磐                 17889\n",
      "蘭                    5984\n",
      "霈                    7449\n",
      "考                    5440\n",
      "##飄                 20654\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dr2gfrRPGILY",
    "outputId": "7bf8552c-16e6-4846-daa7-95bae83c6945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ㄅ', 647)\n",
      "('ㄆ', 648)\n",
      "('ㄇ', 649)\n",
      "('ㄉ', 650)\n",
      "('ㄋ', 651)\n",
      "('ㄌ', 652)\n",
      "('ㄍ', 653)\n",
      "('ㄎ', 654)\n",
      "('ㄏ', 655)\n",
      "('ㄒ', 656)\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(647, 657))\n",
    "some_pairs = [(t, idx) for t, idx in vocab.items() if idx in indices]\n",
    "for pair in some_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ide__gb9GILc",
    "outputId": "ce31c50c-82ea-47c0-fcba-c698796e6614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\n",
      "['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知', '道', '誰', '沒', '穿', '褲', '子', '。'] ...\n",
      "[101, 5023, 1168, 4060, 3717, 103, 749, 8024, 2218, 4761, 6887, 6306, 3760, 4959, 6194, 2094, 511] ...\n"
     ]
    }
   ],
   "source": [
    "#克漏字 [MASK]\n",
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens, '...')\n",
    "print(ids, '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c43utUenGILf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "這段程式碼載入已經訓練好的 masked 語言模型並對有 [MASK] 的句子做預測\n",
    "\"\"\"\n",
    "from transformers import BertForMaskedLM\n",
    "# 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
    "tokens_tensor = torch.tensor([ids])  # (1, seq_len)\n",
    "segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()\n",
    "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
    "maskedLM_model.eval()\n",
    "#當我們在做evaluating的時候（不需要計算導數），\n",
    "#我們可以將推斷（inference）的代碼包裹在with torch.no_grad():之中，以達到暫時不追踪網絡參數中的導數的目的，\n",
    "#總之是為了減少可能存在的計算和內存消耗。\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    # (1, seq_len, num_hidden_units)\n",
    "del maskedLM_model\n",
    "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzqjnccFGILi",
    "outputId": "ec2706f0-3b1e-4540-d334-8bfc75765aa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入 tokens ： ['[CLS]', '等', '到', '潮', '水', '[MASK]', '了', '，', '就', '知'] ...\n",
      "--------------------------------------------------\n",
      "Top 1 (67%)：['[CLS]', '等', '到', '潮', '水', '來', '了', '，', '就', '知'] ...\n",
      "Top 2 (25%)：['[CLS]', '等', '到', '潮', '水', '濕', '了', '，', '就', '知'] ...\n",
      "Top 3 ( 2%)：['[CLS]', '等', '到', '潮', '水', '過', '了', '，', '就', '知'] ...\n"
     ]
    }
   ],
   "source": [
    "# 顯示 top k 可能的字。一般我們就是取 top 1 當作預測值\n",
    "print(\"輸入 tokens ：\", tokens[:10], '...')\n",
    "print('-' * 50)\n",
    "for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "    tokens[masked_index] = t\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:10]), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Suob6bvSGILl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEZOxBmBGILm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at C:\\Users\\88696/.cache\\torch\\transformers\\8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7RpgGiGGILo",
    "outputId": "c19d1978-3460-4e49-b1b5-69ef246e9f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 2657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>苏有朋要结婚了，但网友觉得他还是和林心如比较合适</td>\n",
       "      <td>好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！</td>\n",
       "      <td>李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶</td>\n",
       "      <td>阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>猪油是个宝，一勺猪油等于十副药，先备起来再说</td>\n",
       "      <td>传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>剖析：香椿，为什么会致癌？</td>\n",
       "      <td>香椿含亚硝酸盐多吃会致癌？测完发现是谣言</td>\n",
       "      <td>disagreed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text_a                          text_b      label\n",
       "0       苏有朋要结婚了，但网友觉得他还是和林心如比较合适  好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！  unrelated\n",
       "1  爆料李小璐要成前妻了贾乃亮模仿王宝强一步到位、快刀斩乱麻！  李小璐要变前妻了？贾乃亮可能效仿王宝强当机立断，快刀斩乱麻！     agreed\n",
       "2  为彩礼，母亲把女儿嫁给陌生男子，十年后再见面，母亲湿了眼眶  阿姨，不要彩礼是觉得你家穷，给你台阶下，不要以为我嫁不出去！  unrelated\n",
       "3         猪油是个宝，一勺猪油等于十副药，先备起来再说  传承千百的猪油为何变得人人唯恐避之不及？揭开猪油的四大谣言！  unrelated\n",
       "4                  剖析：香椿，为什么会致癌？            香椿含亚硝酸盐多吃会致癌？测完发现是谣言  disagreed"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 簡單的數據清理，去除空白標題的 examples\n",
    "df_train = pd.read_csv(\"./data/train.csv\")\n",
    "empty_title = ((df_train['title2_zh'].isnull()) \\\n",
    "               | (df_train['title1_zh'].isnull()) \\\n",
    "               | (df_train['title2_zh'] == '') \\\n",
    "               | (df_train['title2_zh'] == '0'))\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "# 剔除過長的樣本以避免 BERT 無法將整個輸入序列放入記憶體不多的 GPU\n",
    "MAX_LENGTH = 30\n",
    "df_train = df_train[~(df_train.title1_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "df_train = df_train[~(df_train.title2_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# 只用 1% 訓練數據看看 BERT 對少量標註數據有多少幫助\n",
    "SAMPLE_FRAC = 0.01\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "# 去除不必要的欄位並重新命名兩標題的欄位名\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']\n",
    "\n",
    "# idempotence, 將處理結果另存成 tsv 供 PyTorch 使用\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"訓練樣本數：\", len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TXYXKwLGILr",
    "outputId": "ad35847d-205f-49a9-9c2c-d472e892e4bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unrelated    0.679338\n",
       "agreed       0.294317\n",
       "disagreed    0.026346\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiqxeQwkGILt",
    "outputId": "3dc2748f-1d84-4481-8b81-9579306ab06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測樣本數： 80126\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>321187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>321190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>321189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>321193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>321191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text_a                       text_b      Id\n",
       "0  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？  321187\n",
       "1              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国  321190\n",
       "2    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思  321189\n",
       "3              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！  321193\n",
       "4              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！  321191"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_test = df_test.loc[:, [\"title1_zh\", \"title2_zh\", \"id\"]]\n",
    "df_test.columns = [\"text_a\", \"text_b\", \"Id\"]\n",
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"預測樣本數：\", len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbZcjdCvGILv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    " \n",
    "    \n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            #test set has no label -> label_tensor = None\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引(0/1/2)方便轉換成 tensor\n",
    "            label_id = self.label_map[label]\n",
    "            #create label tensor\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        # [CLS] [text a] [SEP]\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        # [CLS] [text b] [SEP]\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        #token embeddings -> tokens tensor \n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        #64-bit integer (signed) -> ftype = torch.long\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSDim4H_GILw"
   },
   "outputs": [],
   "source": [
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j65wO1N_GILy",
    "outputId": "c116d08d-dd2b-4597-b6ca-df70f0863650"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
       "         6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
       "         6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
       "         4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
       "         3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZopPe4smGILz",
    "outputId": "a68015ea-ec33-41d1-b79e-dd77641582f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：苏有朋要结婚了，但网友觉得他还是和林心如比较合适\n",
      "句子 2：好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！\n",
      "分類  ：unrelated\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
      "        6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
      "        6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
      "        4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
      "        3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：2\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]苏有朋要结婚了，但网友觉得他还是和林心如比较合适[SEP]好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "#join->將序列中的元素以指定的字符連接生成一個新的字符串\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text_a}\n",
    "句子 2：{text_b}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqoBWUP4GIL1"
   },
   "outputs": [],
   "source": [
    "!pip install pysnooper -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8GORMRcGIL2"
   },
   "outputs": [],
   "source": [
    "import pysnooper\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    #give tensors and text relationship\n",
    "    @pysnooper.snoop()\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            #test set has no label -> label_tensor = None\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引(0/1/2)方便轉換成 tensor\n",
    "            label_id = self.label_map[label]\n",
    "            #create label tensor\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        # [CLS] [text a] [SEP]\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        # [CLS] [text b] [SEP]\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        #token embeddings -> tokens tensor \n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        #64-bit integer (signed) -> ftype = torch.long\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7roz_nV_GIL4"
   },
   "outputs": [],
   "source": [
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset_ = FakeNewsDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8h34SeNYGIL5",
    "outputId": "41f00ef1-3047-4b81-d1a3-7dd0c375900d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source path:... <ipython-input-21-5b8d3b7a1f85>\n",
      "Starting var:.. self = <__main__.FakeNewsDataset object at 0x00000217F81F9308>\n",
      "Starting var:.. idx = 0\n",
      "15:50:12.903786 call        17     def __getitem__(self, idx):\n",
      "15:50:12.903786 line        18         if self.mode == \"test\":\n",
      "15:50:12.904351 line        23             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '苏有朋要结婚了，但网友觉得他还是和林心如比较合适'\n",
      "New var:....... text_b = '好闺蜜结婚给不婚族的秦岚扔花球，倒霉的秦岚掉水里笑哭苏有朋！'\n",
      "New var:....... label = 'unrelated'\n",
      "15:50:12.904776 line        25             label_id = self.label_map[label]\n",
      "New var:....... label_id = 2\n",
      "15:50:12.905779 line        27             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(2)\n",
      "15:50:12.906054 line        30         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "15:50:12.906293 line        31         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['苏', '有', '朋', '要', '结', '婚', '了', '，', '但', '网...'还', '是', '和', '林', '心', '如', '比', '较', '合', '适']\n",
      "15:50:12.906771 line        33         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '和', '林', '心', '如', '比', '较', '合', '适', '[SEP]']\n",
      "15:50:12.906771 line        34         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "15:50:12.906771 line        37         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['好', '闺', '蜜', '结', '婚', '给', '不', '婚', '族', '的...'岚', '掉', '水', '里', '笑', '哭', '苏', '有', '朋', '！']\n",
      "15:50:12.907767 line        39         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苏', '有', '朋', '要', '结', '婚', '了', '，'... '水', '里', '笑', '哭', '苏', '有', '朋', '！', '[SEP]']\n",
      "15:50:12.908809 line        40         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "15:50:12.908809 line        43         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5722, 3300, 3301, 6206, 5310, 2042, 749, 8...7, 7027, 5010, 1526, 5722, 3300, 3301, 8013, 102]\n",
      "15:50:12.910130 line        45         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042... 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102])\n",
      "15:50:12.910473 line        49         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "15:50:12.910971 line        50                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "15:50:12.912936 line        52         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "15:50:12.917251 return      52         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5722, 3300, 3301, 6206, 5310, 204...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(2))\n",
      "Elapsed time: 00:00:00.016612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 5722, 3300, 3301, 6206, 5310, 2042,  749, 8024,  852, 5381, 1351,\n",
       "         6230, 2533,  800, 6820, 3221, 1469, 3360, 2552, 1963, 3683, 6772, 1394,\n",
       "         6844,  102, 1962, 7318, 6057, 5310, 2042, 5314,  679, 2042, 3184, 4638,\n",
       "         4912, 2269, 2803, 5709, 4413, 8024,  948, 7450, 4638, 4912, 2269, 2957,\n",
       "         3717, 7027, 5010, 1526, 5722, 3300, 3301, 8013,  102]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4InUyUOVGIL7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels -> None\n",
    "    #not None -> train set\n",
    "    if samples[0][2] is not None:\n",
    "        #Concatenates sequence of tensors along a new(default:0) dimension.\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度(batch_first=True)\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7jTQyWMGIL-",
    "outputId": "261921ee-9764-4b1e-94a9-e3aec22ba176"
   },
   "outputs": [],
   "source": [
    "trainloader2 = DataLoader(trainset, batch_size=16, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WCkhsUUGIMA",
    "outputId": "a4341880-7b06-4aae-fd2e-ad87cc154356",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 5722, 3300,  ...,    0,    0,    0],\n",
       "         [ 101, 4255, 3160,  ..., 8013,  102,    0],\n",
       "         [ 101,  711, 2506,  ..., 8013,  102,    0],\n",
       "         ...,\n",
       "         [ 101,  671, 2157,  ...,    0,    0,    0],\n",
       "         [ 101, 1380,  677,  ...,    0,    0,    0],\n",
       "         [ 101, 2458, 1853,  ...,    0,    0,    0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([2, 0, 2, 2, 1, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "         2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0,\n",
       "         0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成跌代器(iter)的下一個項目\n",
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1k61uGSGIMB",
    "outputId": "306d6076-4190-4505-fb41-3aa06929c5cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 63]) \n",
      "tensor([[ 101, 5722, 3300,  ...,    0,    0,    0],\n",
      "        [ 101, 4255, 3160,  ..., 8013,  102,    0],\n",
      "        [ 101,  711, 2506,  ..., 8013,  102,    0],\n",
      "        ...,\n",
      "        [ 101,  671, 2157,  ...,    0,    0,    0],\n",
      "        [ 101, 1380,  677,  ...,    0,    0,    0],\n",
      "        [ 101, 2458, 1853,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 63])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 63])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([2, 0, 2, 2, 1, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0,\n",
      "        0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_XRoTfZGIMD"
   },
   "source": [
    "# 在 BERT 之上加入新 layer 成下游任務模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F25tWEjXGIMD",
    "outputId": "985bdcc8-5a29-4896-aae2-183fe014010d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 載入一個可以做中文多分類任務的模型，n_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfyZjA4mGIME",
    "outputId": "f0161f6c-aad7-413b-d10e-d430b2eaa33f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#你也可以看到整個分類模型 model 預設的隱狀態維度(hidden size)為 768。\n",
    "#如果你想要更改 BERT 的超參數，可以透過給一個 config dict 來設定。\n",
    "#以下則是分類模型 model 預設的參數設定：\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsdYLTOnGIMH"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
    "\n",
    "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    #暫時不追踪網絡參數中的導數的目的，總之是為了減少可能存在的計算和內存消耗。  \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            #1->列max\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BH83UPKkGIMI",
    "outputId": "ca894997-d31f-42da-eeba-07be6d894ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 0.026345502446368085\n"
     ]
    }
   ],
   "source": [
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkz2todxGIMK",
    "outputId": "8034dd0e-be9a-4c6e-b8f2-98254089d40b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0'), 0.026345502446368085)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(model, trainloader, compute_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aDl-8q_GIML"
   },
   "outputs": [],
   "source": [
    "#毫不意外，模型裡新加的線性分類器才剛剛被初始化，\n",
    "#整個分類模型的表現低於 68 % 的 baseline 是非常正常的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWXIpe0yGIMN"
   },
   "outputs": [],
   "source": [
    "#算模型的參數量\n",
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evOFEe_nGIMP",
    "outputId": "642f48ea-e28b-4aab-96b8-df06c7c6d686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：102269955\n",
      "線性分類器的參數量：2307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "#numel: 用來返回net網絡中的參數\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkhevM2uGIMQ"
   },
   "source": [
    "#  訓練該下游任務模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-udUbRlOGIMQ",
    "outputId": "43ba7033-2732-4a02-bde1-4e6eb616f3bb",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 4.00 GiB total capacity; 2.98 GiB already allocated; 3.01 MiB free; 3.08 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m         )\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m         )\n\u001b[0;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[1;32m--> 407\u001b[1;33m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m             )\n\u001b[0;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     ):\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mself_attention_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add self attentions if we output attention weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    312\u001b[0m     ):\n\u001b[0;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         )\n\u001b[0;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_19\\envs\\PythonGPU\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 4.00 GiB total capacity; 2.98 GiB already allocated; 3.01 MiB free; 3.08 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 6  # 幸運數字\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJq4CElsGIMT",
    "outputId": "5478e13b-f0b8-4903-8010-5c99e3a8cf19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Name', 'Runoob'), ('Age', 7)])"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ = {'Name': 'Runoob', 'Age': 7}\n",
    "dict_.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cw_RCwKKGIMW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "N_XRoTfZGIMD"
   ],
   "name": "NLP_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
