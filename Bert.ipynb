{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "7OyCLf8b9EDL",
    "outputId": "f33497da-f041-443a-8b90-b67e4a32d012"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#NewsDataset class：將url中的Data轉成Bert input型態，包含tokens、masks、labels\n",
    "#tokens：可以理解為字元向量，因為有作padding，也就是讓各向量長度相同，補0之動作(padding主要是讓向量們可以使用矩陣運算，加快訓練速度)\n",
    "#masks ：用於區分是否為padding之element，tokens為1、paddings為0\n",
    "#labels：資料一開始的標籤，也就是用於訓練的答案\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, mode, url, tokenizer):\n",
    "        assert mode in [\"train\", \"predict\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(url, delimiter='\\t', header=None)\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        \n",
    "#-------you might adjust codes here!!!!!!!------reset indices-----With different input data, codes here should be modified correspondingly\n",
    "        self.df = self.df.reset_index()\n",
    "        self.df = self.df.loc[:, [0, 1]]\n",
    "        self.df.columns = ['text', 'label']\n",
    "        self.text =self.df['text'].values\n",
    "        self.labels = self.df['label'].values\n",
    "        \n",
    "        #-------------convert to Bert-pretrained tokens-----------------\n",
    "        self.tokens = []\n",
    "        for sent in self.text:\n",
    "        # `encode` will:(1) Tokenize the sentence.(2) Prepend the `[CLS]` token to the start. (3) Append the `[SEP]` token to the end.(4) Map tokens to their IDs.\n",
    "           encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "           self.tokens.append(encoded_sent)\n",
    "        \n",
    "        #-------------padding-------------------------------------------\n",
    "        MAX_LEN = max(len(self.tokens[i]) for i in range(len(self.tokens)))\n",
    "        self.tokens = pad_sequences(self.tokens, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "        #-------------Create attention masks----------------------------\n",
    "        attention_masks = []\n",
    "        # Create a mask of 1s for each token followed by 0s for padding\n",
    "        for seq in self.tokens:\n",
    "           mask = [int(i>0) for i in seq]\n",
    "           attention_masks.append(mask) \n",
    "\n",
    "        #-------------Convert to tensors--------------------------------\n",
    "        self.tokens = torch.tensor(self.tokens)\n",
    "        self.masks = torch.tensor(attention_masks)\n",
    "        self.labels = torch.tensor(self.labels)       \n",
    "\n",
    "\n",
    "    #---------inherit from Dataset needing define __len__ and __getitem__ methods--------------\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "       if self.mode == \"predict\":\n",
    "            token = self.tokens[idx]\n",
    "            mask =self.masks[idx]\n",
    "            label_tensor = None\n",
    "       else:\n",
    "            token = self.tokens[idx]\n",
    "            mask =self.masks[idx]\n",
    "            label = self.labels[idx]\n",
    "       return (token, mask, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEgaI5cXbRa0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def data_split(data_set,test_size=0.1,batch_size=20):\n",
    "    #-------------------Use 90% for training and 10% for validation.\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(data_set.tokens, data_set.labels, random_state=2020, test_size=test_size)\n",
    "    #-------------------Do the same for the masks.\n",
    "    train_masks, validation_masks, _, _ = train_test_split(data_set.masks, data_set.labels, random_state=2020, test_size=test_size)\n",
    "\n",
    "    # Create the DataLoader for our training set.\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # Create the DataLoader for our validation set.\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    return train_dataloader, validation_dataloader\n",
    "    \n",
    "#data_split[0] is train_dataloader while data_split[1] is validation_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBmJx9pifsjo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def trainprocess(model, train_dataloader, validation_dataloader):\n",
    "    #---------------If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    #---------------If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "    # Number of training epochs (authors recommend between 2 and 4)\n",
    "    epochs = 4\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "\n",
    "    # This training code is based on the `run_glue.py` script here:\n",
    "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    # with the same seed value, one can produce same random list in every execution\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "    \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('================== Epoch {:} / {:} =================='.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into training mode. \n",
    "        # Don't be mislead--the call to \"train\" just changes the *mode*, it doesn't *perform* the training.\n",
    "        # \"dropout\" and \"batchnorm\" layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "    \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches and show execution time for each step\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \"to\" method.\n",
    "            # batch contains three pytorch tensors:\n",
    "            #   [0]: input tokens \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].type(torch.LongTensor)\n",
    "            b_input_mask = batch[1].type(torch.LongTensor)\n",
    "            b_labels = batch[2].type(torch.LongTensor)\n",
    "        \n",
    "                    \n",
    "            b_input_ids =  b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()\n",
    "\n",
    "        \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we have provided the \"labels\".\n",
    "            # The documentation for this \"model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "               \n",
    "            # The call to \"model\" always returns a tuple, so we need to pull the loss value out of the tuple.\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n",
    "            # \"loss\" is a Tensor containing a single value; the \".item()\" function just returns the Python value from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Add batch to GPU\n",
    "            #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "            # Unpack the inputs from our dataloader\n",
    "            #b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = batch[0].type(torch.LongTensor)\n",
    "            b_input_mask = batch[1].type(torch.LongTensor)\n",
    "            b_labels = batch[2].type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "            b_input_ids =  b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have not provided labels.\n",
    "                # token_type_ids is the same as the \"segment ids\", which differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this \"model\" function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the outputvalues prior to applying an activation function like the softmax.\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            # Track the number of batches\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"memory used:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "    print(\"cache used:{:}\".format(torch.cuda.memory_cached(device=0)))\n",
    "    del optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(\"memory used:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "    print(\"cache used:{:}\".format(torch.cuda.memory_cached(device=0)))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AN1I2-4Sva1t",
    "outputId": "d1b4a7c6-8b7b-45bb-b03b-bcce917c3f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory used:0\n",
      "cache used:0\n",
      "memory:0\n",
      "cache:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc, os\n",
    "import jupyter_client\n",
    "print(\"memory used:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "print(\"cache used:{:}\".format(torch.cuda.memory_cached(device=0)))\n",
    "\n",
    "#del model\n",
    "torch.cuda.reset_max_memory_cached(device=0)\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()\n",
    "#a=torch.cuda.memory_stats(device=0)\n",
    "torch.cuda.reset_max_memory_allocated(device=0)\n",
    "\n",
    "jupyter_client.KernelManager.shutdown_kernel\n",
    "os._exit\n",
    "print(\"memory:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "print(\"cache:{:}\".format(torch.cuda.memory_cached(device=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoWYh19HGkHV"
   },
   "outputs": [],
   "source": [
    "def model_prediction(model, tokenizer, sentences):\n",
    "    #---------------If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    #---------------If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "   \n",
    "    encoded_token=[]\n",
    "    for sent in sentences:\n",
    "      encoded_sent = tokenizer.encode(sent,add_special_tokens = True)\n",
    "      encoded_token.append(encoded_sent)\n",
    " \n",
    "    MAX_LEN = max(len(sent) for sent in encoded_token)\n",
    "    encoded_id = pad_sequences(encoded_token, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    encoded_id=torch.tensor(encoded_id)  \n",
    "    \n",
    "      \n",
    "    model.eval()   \n",
    "    b_input_ids = encoded_id.type(torch.LongTensor)\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():             \n",
    "        outputs = model(b_input_ids)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        print(outputs[0])\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result=np.argmax(logits, axis=1).flatten()\n",
    "        print(result)\n",
    "\n",
    "import os\n",
    "#Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "def save_model(model, tokenizer, output_dir):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c2966ef1b14fa2a2e5fd4dfba3e3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184502ef84ea4ea68391f8bbdb954ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model_version = 'bert-base-uncased'   #此處可以更改訓練語言，中文pretrain model為'bert-base-chinese'\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_version,          # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2,               # The number of output labels--2 for binary classification and you can increase this for multi-class tasks.   \n",
    "    output_attentions = False,    # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCQaRe5hGkHa",
    "outputId": "dae55912-4cb6-48a2-e73e-6a10aff0e86f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 965M\n",
      "\n",
      "================== Epoch 1 / 4 ==================\n",
      "Training...\n",
      "  Batch    40  of    312.    Elapsed: 0:00:36.\n",
      "  Batch    80  of    312.    Elapsed: 0:01:07.\n",
      "  Batch   120  of    312.    Elapsed: 0:01:40.\n",
      "  Batch   160  of    312.    Elapsed: 0:02:12.\n",
      "  Batch   200  of    312.    Elapsed: 0:02:46.\n",
      "  Batch   240  of    312.    Elapsed: 0:03:19.\n",
      "  Batch   280  of    312.    Elapsed: 0:03:53.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epcoh took: 0:04:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "================== Epoch 2 / 4 ==================\n",
      "Training...\n",
      "  Batch    40  of    312.    Elapsed: 0:00:33.\n",
      "  Batch    80  of    312.    Elapsed: 0:01:07.\n",
      "  Batch   120  of    312.    Elapsed: 0:01:40.\n",
      "  Batch   160  of    312.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    312.    Elapsed: 0:02:47.\n",
      "  Batch   240  of    312.    Elapsed: 0:03:20.\n",
      "  Batch   280  of    312.    Elapsed: 0:03:53.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:04:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.89\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "================== Epoch 3 / 4 ==================\n",
      "Training...\n",
      "  Batch    40  of    312.    Elapsed: 0:00:35.\n",
      "  Batch    80  of    312.    Elapsed: 0:01:09.\n",
      "  Batch   120  of    312.    Elapsed: 0:01:42.\n",
      "  Batch   160  of    312.    Elapsed: 0:02:16.\n",
      "  Batch   200  of    312.    Elapsed: 0:02:49.\n",
      "  Batch   240  of    312.    Elapsed: 0:03:23.\n",
      "  Batch   280  of    312.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:04:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "================== Epoch 4 / 4 ==================\n",
      "Training...\n",
      "  Batch    40  of    312.    Elapsed: 0:00:36.\n",
      "  Batch    80  of    312.    Elapsed: 0:01:09.\n",
      "  Batch   120  of    312.    Elapsed: 0:01:43.\n",
      "  Batch   160  of    312.    Elapsed: 0:02:16.\n",
      "  Batch   200  of    312.    Elapsed: 0:02:49.\n",
      "  Batch   240  of    312.    Elapsed: 0:03:23.\n",
      "  Batch   280  of    312.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:04:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation took: 0:00:08\n",
      "\n",
      "Training complete!\n",
      "memory used:1765995008\n",
      "cache used:3007315968\n",
      "memory used:881861632\n",
      "cache used:1149239296\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 965M\n",
      "tensor([[ 2.8254, -3.0043],\n",
      "        [ 2.9773, -3.4833],\n",
      "        [ 3.0392, -3.5904],\n",
      "        [-1.8015,  2.3009]], device='cuda:0')\n",
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#from transformers import BertTokenizer, BertForSequenceClassification\n",
    "#model_version = 'C:/Users/vtteam/Documents/bert-base-uncased'   #此處可以更改訓練語言，中文pretrain model為'bert-base-chinese'\n",
    "## Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "#model = BertForSequenceClassification.from_pretrained(\n",
    "#    model_version,          # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#    num_labels = 2,               # The number of output labels--2 for binary classification and you can increase this for multi-class tasks.   \n",
    "#    output_attentions = False,    # Whether the model returns attentions weights.\n",
    "#    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "#)\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "\n",
    "#-------------------you can use either data url or it's path as input-----------------------------------------------\n",
    "data_set_url='https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    "#----you might need to adjust the codes in NewsDataSet class depending on which columns in CSV are training contents and labels\n",
    "data_set = NewsDataset(\"train\", data_set_url, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#----split your data as training set and validation set-------------------------------------------------------------\n",
    "#----data_split(data_set,test_size=0.1,batch_size=20) as you can see test_size and batch_size have default values---\n",
    "dataloader = data_split(data_set)\n",
    "train_dataloader =dataloader[0]\n",
    "validation_dataloader=dataloader[1]\n",
    "\n",
    "\n",
    "#----train!!!!!------------------------------------------------------------------------------------------------------\n",
    "model=trainprocess(model,train_dataloader,validation_dataloader)\n",
    "\n",
    "\n",
    "#----use model to predict--------------------------------------------------------------------------------------------\n",
    "sentences =['South Korea’s Kospi led losses among the region’s major markets as it dropped 6.86% while the Kosdaq index fell 7.5%.',\n",
    "            'Stocks tumbled on Wednesday, reaching a new coronavirus crisis low as investors worried about the economic damage from the pandemic.',\n",
    "            'That is a terrible movie',\n",
    "            'Dow rebounds more than 1,000 points as Trump seeks $1 trillion in stimulus for coronavirus fight']\n",
    "model_prediction(model, tokenizer, sentences)\n",
    "\n",
    "\n",
    "#----save trained-model-----------------------------------------------------------------------------------------------\n",
    "output_dir = './model_save/'\n",
    "#save_model(model, tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6RhZFmKGkHd"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKU8RUMRGkHh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
