{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mstats\n",
    "#from progressbar import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./Stats_Detail_New/'):\n",
    "    os.makedirs('./Stats_Detail_New/')\n",
    "if not os.path.exists('./Stats_Detail_New/multiway'):\n",
    "    os.makedirs('./Stats_Detail_New/multiway')    \n",
    "    \n",
    "if not os.path.exists('./Weight/multiway'):\n",
    "    os.makedirs('./Weight/multiway')\n",
    "if not os.path.exists('./Graph_new/multiway'):\n",
    "    os.makedirs('./Graph_new/multiway')\n",
    "if not os.path.exists('./stats/multiway'):\n",
    "    os.makedirs('./stats/multiway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指標設定function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#數量前10大產業\n",
    "def select_top10_ind(data):\n",
    "    data_MV = data[data.MV_Select == 1]\n",
    "    #倒敘排列大小順序\n",
    "    select_ind = data_MV.groupby('Ind')['Code'].count().sort_values(ascending = False) \n",
    "    # why need to > 4?? -> 剔除少於4檔個股的產業\n",
    "    select_ind = select_ind[select_ind > 4]\n",
    "    # 前10大之Ind -> to list\n",
    "    select_ind = select_ind.index[0:10].tolist()\n",
    "    #return 1 or 0\n",
    "    return [ ((x in select_ind) & ( y in data_MV['Code'].tolist())) * 1 \n",
    "            for x, y in zip(data['Ind'], data['Code'])]\n",
    "\n",
    "#some setting to calculate 因子值加權法\n",
    "#極端值調整平均數 -> 去極值\n",
    "#if s is all nan -> return s\n",
    "#else -> 調整後1%、前1%的數值\n",
    "def winsorize(s):\n",
    "    if  sum(pd.isna(s) == True) == len(s):\n",
    "        return s\n",
    "    else:   \n",
    "        return mstats.winsorize(s, limits=[0.01, 0.01])\n",
    "\n",
    "#normalize to 0~1 range\n",
    "def min_max(s):\n",
    "    if sum(pd.isna(s) == True) == len(s):\n",
    "        return s\n",
    "    else:   \n",
    "        return (s - np.min(s)) / (np.max(s) - np.min(s))\n",
    "\n",
    "#標準化    \n",
    "def stardize(s):\n",
    "    if sum(pd.isna(s)==True)==len(s):\n",
    "        return s\n",
    "    else:   \n",
    "        return (s-np.mean(s))/ np.std(s)  \n",
    "\n",
    "#調整極端值後，進行normalize to 0~1    \n",
    "def min_max_win(df,factor):\n",
    "    df[factor+'_t-1'] = winsorize(df[factor+'_t-1'])\n",
    "    df[factor+'_t-1'] = min_max(df[factor+'_t-1'])\n",
    "    return df[factor+'_t-1']\n",
    "\n",
    "#調整極端值後，進行標準化\n",
    "def standardize_win(df,factor):\n",
    "    df[factor+'_t-1'] = winsorize(df[factor+'_t-1'])\n",
    "    df[factor+'_t-1'] = stardize(df[factor+'_t-1'])\n",
    "    return df[factor+'_t-1']\n",
    "\n",
    "#持有總值(Amount)、numbers of stocks(除以1000)\n",
    "def Cal_Amount(df,factor,weight_method):\n",
    "    df[factor+'_Nstock_'+weight_method] = [initial_account/2*x/y/1000 for x,y in zip(df[factor+'_Weight_'+weight_method],\n",
    "                                                                                     df['Close'])]\n",
    "    df[factor+'_Nstock_'+weight_method] = df[factor+'_Nstock_'+weight_method].apply(np.ceil)\n",
    "    df[factor+'_Amount_'+weight_method] = df[factor+'_Nstock_'+weight_method]*df['Close']*1000    \n",
    "    return df[factor+'_Nstock_'+weight_method],df[factor+'_Amount_'+weight_method]\n",
    "#Profit\n",
    "def Cal_Profit(df, factor, weight_method):\n",
    "    df[factor+'_Profit_'+weight_method] = ((df['Close_t+1']-df['Close']) * df[factor+'_Nstock_'+weight_method])*1000\n",
    "    # ex: 'short_index' in 'tri_short_index' -> True\n",
    "    if 'short_index' in weight_method:\n",
    "        #Rm -> 台灣發行量加權股價報酬指數 pct change\n",
    "        Rm = df['Rm'].mode()\n",
    "        Profit = df[factor + '_Profit_' + weight_method].sum() - initial_account * Rm/2\n",
    "    elif 'long index' in weight_method:\n",
    "        Rm = df['Rm'].mode()\n",
    "        Profit = df[factor + '_Profit_' + weight_method].sum() + initial_account * Rm/2\n",
    "    else:\n",
    "        Profit = df[factor+'_Profit_'+weight_method].sum()\n",
    "    return Profit\n",
    "\n",
    "#Tax Cost\n",
    "def Cal_Cost(df, factor, weight_method):\n",
    "    #今天手上的股票數量-昨天的，再*股價\n",
    "    holdsell_shortmore = df['Close']*(df[factor+'_Nstock_'+weight_method] - df[factor+'_Nstock_'+weight_method+'_t-1'])\n",
    "    #今天的量比昨天少(<0) -> 賣出股票 -> tax\n",
    "    holdsell_shortmore = abs(holdsell_shortmore[holdsell_shortmore < 0])\n",
    "    df[factor + '_Tax_' + weight_method] = holdsell_shortmore\n",
    "    trade_tax = np.sum(holdsell_shortmore*0.003*1000)\n",
    "    return trade_tax\n",
    "\n",
    "#Turnover(周轉率)\n",
    "def Cal_Turnover(df,factor,weight_method):\n",
    "    Turnover = np.sum(abs(df['Close']*(df[factor+'_Nstock_'+weight_method] - df[factor+'_Nstock_'+weight_method+'_t-1'])))*1000\n",
    "    return Turnover\n",
    "\n",
    "#計算權重\n",
    "#1.排序權重法\n",
    "#因子值加權、因子值排序加權\n",
    "def cal_3group_weight_tri(df, factor, upper_pec, lower_pec, longbig, short_index=False, long_index=False, value=False):\n",
    "    df = df.sort_values(factor+'_t-1', ascending=False)\n",
    "    #給予排序\n",
    "    df[factor+'_Rank'] = np.arange(len(df)) + 1\n",
    "    length = len(df)\n",
    "    middle = np.ceil(length /2)\n",
    "    #因子值加權\n",
    "    if value == True:\n",
    "        #先去極值、標準化\n",
    "        df[factor+'_t-1'] = winsorize(df[factor+'_t-1'])\n",
    "        df[factor+'_t-1'] = min_max(df[factor+'_t-1'])\n",
    "        #從中間切一半，分成large(1~100)、small(101~200) -> PE_t-1\n",
    "        large_Rank = df[df[factor+'_Rank'] <= middle][factor+'_t-1']\n",
    "        small_Rank = df[df[factor+'_Rank'] > middle][factor+'_t-1']\n",
    "        #都為正數\n",
    "        large_Rank = large_Rank    \n",
    "        small_Rank = 1 - small_Rank\n",
    "    \n",
    "    #因子值排序加權\n",
    "    else:                     \n",
    "        #從中間切一半，分成large(1~100)、small(101~200) -> PE_Rank\n",
    "        large_Rank = df[df[factor+'_Rank'] <= middle][factor+'_Rank'] \n",
    "        small_Rank = df[df[factor+'_Rank'] > middle][factor+'_Rank']\n",
    "        #數值最大的逆著排 -> 100 99 98....1   #最大是一\n",
    "        large_Rank = middle + 1 - large_Rank\n",
    "        #因子數值最小的順著排 -> 1 2 3....100 #最小是一\n",
    "        small_Rank = small_Rank - middle                             \n",
    "    #買因子大的，賣因子小的 (一買一賣 -> delta對消的概念)\n",
    "    if longbig == 1:\n",
    "        #買大因子且放空指數 -> 因子小的權重為0\n",
    "        if short_index == True:\n",
    "            large_Rank = large_Rank / (large_Rank.sum())  \n",
    "            small_Rank = small_Rank / (small_Rank.sum()) * 0 \n",
    "        #賣小因子且做多指數 -> 因子大的權重為0\n",
    "        elif long_index == True:\n",
    "            large_Rank = large_Rank / (large_Rank.sum()) * 0\n",
    "            small_Rank = small_Rank / (small_Rank.sum())              \n",
    "        #不做指數，因此large、small各一半\n",
    "        else:\n",
    "            large_Rank = large_Rank / (large_Rank.sum())  \n",
    "            small_Rank = small_Rank / (small_Rank.sum())             \n",
    "        return pd.concat([large_Rank, -small_Rank]), df[factor+'_Rank'] \n",
    "    #賣因子大的，買因子小的\n",
    "    else:\n",
    "        #買小因子，且放空指數 -> 因子大的權重為0\n",
    "        if short_index == True: \n",
    "            large_Rank = large_Rank / (large_Rank.sum()) * 0 \n",
    "            small_Rank = small_Rank / (small_Rank.sum()) \n",
    "        #賣大因子，且做多指數 -> 因子小的權重為0\n",
    "        elif long_index == True:\n",
    "            large_Rank = large_Rank / (large_Rank.sum()) \n",
    "            small_Rank = small_Rank / (small_Rank.sum()) * 0 \n",
    "        #不做指數，因此large、small各一半\n",
    "        else:\n",
    "            large_Rank = large_Rank / (large_Rank.sum())  \n",
    "            small_Rank = small_Rank / (small_Rank.sum())  \n",
    "        return  pd.concat([-large_Rank, small_Rank]), df[factor+'_Rank']\n",
    "\n",
    "#2.平均權重法\n",
    "#因子值加權、因子值排序加權\n",
    "def cal_3group_weight_eq(df, factor, upper_pec, lower_pec, longbig, ind=False, short_index=False, long_index=False, value=False):\n",
    "    df = df.sort_values(factor+'_t-1', ascending=False)\n",
    "    df[factor+'_Rank'] = np.arange(len(df)) + 1    \n",
    "    length = len(df)\n",
    "    if ind == False:\n",
    "        #upper、lower pecentage -> 看要取因子最大、最小的前幾% ex: 0.1、0.9\n",
    "        upper = int(length*upper_pec)\n",
    "        #numbers of upper\n",
    "        N_upper = upper\n",
    "        lower = int(length*lower_pec)\n",
    "        #numbers of lower\n",
    "        N_lower = length - lower\n",
    "    #long/short因子值最大的2檔個股，並short/long因子值最小的2檔個股 -> eq_ind特有\n",
    "    else:\n",
    "        upper = 2\n",
    "        lower = len(df) - 2\n",
    "    #因子值加權\n",
    "    if value == True: \n",
    "        #先去極值、標準化\n",
    "        df[factor+'_t-1'] = winsorize(df[factor+'_t-1'])\n",
    "        df[factor+'_t-1'] = min_max(df[factor+'_t-1'])\n",
    "        #large rank -> 前幾%的PE_t-1 ex:取rank <= 200*0.1=20 -> 等同於取前10%\n",
    "        large_Rank = df[df[factor+'_Rank'] <= upper][factor+'_t-1']\n",
    "        #middle rank ?? -> 不配給權重\n",
    "        middle_Rank = df[(lower >= df[factor+'_Rank'])&(df[factor+'_Rank'] > upper)][factor+'_t-1']*0\n",
    "        #small rank -> ex:200*0.9 ->180， 取rank >180 -> 等同於取後10%\n",
    "        small_Rank = df[df[factor+'_Rank'] > lower][factor+'_t-1']\n",
    "        large_Rank = large_Rank\n",
    "        large_Rank = large_Rank / large_Rank.sum()\n",
    "        middle_Rank = middle_Rank * 0\n",
    "        small_Rank = 1 - small_Rank \n",
    "        small_Rank = small_Rank / small_Rank.sum()\n",
    "    #因子值排序加權\n",
    "    else:            \n",
    "        #取前10%的rank\n",
    "        large_Rank = df[df[factor+'_Rank'] <= upper][factor+'_Rank']\n",
    "        #equal weighted\n",
    "        large_Rank = large_Rank/large_Rank/len(large_Rank)\n",
    "        #中間的 ex:21~180，這裡不配給權重\n",
    "        middle_Rank = df[(lower>=df[factor+'_Rank'])&(df[factor+'_Rank'] > upper)][factor+'_Rank']\n",
    "        #取後10%的rank\n",
    "        small_Rank = df[df[factor+'_Rank'] > lower][factor+'_Rank']\n",
    "        #equal weighted\n",
    "        small_Rank = small_Rank/small_Rank/len(small_Rank)            \n",
    "    #買因子大的，賣因子小的 (一買一賣 -> delta對消的概念)\n",
    "    if longbig == 1:\n",
    "        #買大因子且放空指數 -> 因子小的權重為0\n",
    "        if short_index == True:\n",
    "            large_Rank  = large_Rank\n",
    "            middle_Rank = middle_Rank  *0\n",
    "            small_Rank  = small_Rank   *0    \n",
    "        #賣小因子且做多指數 -> 因子大的權重為0\n",
    "        elif long_index == True:\n",
    "            large_Rank  = large_Rank   *0\n",
    "            middle_Rank = middle_Rank  *0\n",
    "            small_Rank  = small_Rank                   \n",
    "        #不做指數，因此large、small各一半\n",
    "        else:\n",
    "            large_Rank  = large_Rank\n",
    "            middle_Rank = middle_Rank  *0\n",
    "            small_Rank  = small_Rank\n",
    "        return pd.concat([large_Rank, middle_Rank, -small_Rank]), df[factor+'_Rank']\n",
    "    #賣因子大的，買因子小的\n",
    "    else:         \n",
    "        #買小因子，且放空指數 -> 因子大的權重為0\n",
    "        if short_index == True:\n",
    "            large_Rank  = large_Rank   *0   \n",
    "            middle_Rank = middle_Rank  *0\n",
    "            small_Rank  = small_Rank\n",
    "        #賣大因子，且做多指數 -> 因子小的權重為0\n",
    "        elif long_index == True:\n",
    "            large_Rank  = large_Rank      \n",
    "            middle_Rank = middle_Rank  *0\n",
    "            small_Rank  = small_Rank   *0         \n",
    "        #不做指數，因此large、small各一半\n",
    "        else:\n",
    "            large_Rank  = large_Rank\n",
    "            middle_Rank = middle_Rank  *0\n",
    "            small_Rank  = small_Rank\n",
    "        return pd.concat([-large_Rank, middle_Rank, small_Rank]), df[factor+'_Rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main strategy function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cal_Weight(weight_method, df_MV, df_Ind, factor, longbig, Factor_args):\n",
    "    #df_Ind ->前10大產業，此外並剔除少於4檔個股的產業\n",
    "    less_stock_Ind = (df_Ind.groupby('Ind')[factor+'_t-1'].count() > 4).index.tolist()\n",
    "    df_Ind = df_Ind[[x in less_stock_Ind for x in df_Ind['Ind']]]\n",
    "    #共有幾個產業符合標準\n",
    "    N_Ind_Select  = len(df_Ind['Ind'].unique()) \n",
    "    #依據權重選取方法使用上下界參數(eq or tri)\n",
    "    if 'tri' in weight_method:                  \n",
    "        upper_pec, lower_pec = Factor_args['upper_lower_perc_tri']['upper'], Factor_args['upper_lower_perc_tri']['lower']\n",
    "    else:\n",
    "        upper_pec, lower_pec = Factor_args['upper_lower_perc_eq']['upper'], Factor_args['upper_lower_perc_eq']['lower']\n",
    "    \n",
    "    ## main strategy\n",
    "    ## 因子值排序加權\n",
    "    #排序權重法\n",
    "    if weight_method == 'tri':\n",
    "        df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = cal_3group_weight_tri(df_MV, factor, upper_pec, lower_pec, longbig)\n",
    "    #排序權重法(考慮前10大產業產業)\n",
    "    elif weight_method == 'tri_ind':\n",
    "        #依照各個產業去分別分配權重 -> groupby('Ind')\n",
    "        weight = df_Ind.groupby('Ind', as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec,\n",
    "                                                                                             lower_pec,longbig)[0])\n",
    "        #依照各個產業去分別排序\n",
    "        rank = df_Ind.groupby('Ind', as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec,\n",
    "                                                                                             lower_pec,longbig)[1])\n",
    "        #上述求出的weight，再除回各個產業的個數 -> 真正的weight \n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #排序權重法(考慮產業、產業市值) \n",
    "    elif weight_method == 'tri_ind_cap' :\n",
    "        weight = df_Ind.groupby('Ind', as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec,\n",
    "                                                                                             lower_pec,longbig)[0]) \n",
    "        rank = df_Ind.groupby('Ind', as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec,\n",
    "                                                                                             lower_pec,longbig)[1])        \n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "        #最後 * 之前算出的Ind_CAP_Weight -> 依據產業市值分配產業權重 \n",
    "        df_MV[factor+'_Weight_'+weight_method] = df_MV[factor+'_Weight_'+weight_method]*df_MV[\"Ind_CAP_Weight\"]\n",
    "    \n",
    "    #平均權重法\n",
    "    elif weight_method == 'eq':\n",
    "        df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = cal_3group_weight_eq(df_MV,factor,upper_pec,lower_pec,longbig\n",
    "                                                                                            ,ind=False)\n",
    "    #平均權重法(考慮前10大產業)\n",
    "    elif weight_method == 'eq_ind':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor,upper_pec,\n",
    "                                                                                            lower_pec,longbig,ind=True)[0] )\n",
    "        rank = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor,upper_pec,\n",
    "                                                                                            lower_pec,longbig,ind=True)[1] )\n",
    "        #上述求出的weight，再除回各個產業的個數 -> 真正的weight \n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)  \n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    \n",
    "    #放空指數區\n",
    "    #tri + short\n",
    "    elif weight_method == 'tri_short_index':\n",
    "        df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = cal_3group_weight_tri(df_MV, factor, upper_pec, lower_pec\n",
    "                                                                                             , longbig, short_index=True)\n",
    "    #tri_ind + short\n",
    "    elif weight_method == 'tri_ind_short_index':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec\n",
    "                                                                                                            ,lower_pec,longbig,short_index=True)[0])  \n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec\n",
    "                                                                                                            ,lower_pec,longbig,short_index=True)[1])   \n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #tri_ind_cap + short \n",
    "    elif weight_method == 'tri_ind_cap_short_index':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                        ,upper_pec,lower_pec,longbig,short_index=True)[0])  \n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                        ,upper_pec,lower_pec,longbig,short_index=True)[1])   \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Weight_'+weight_method] = df_MV[factor+'_Weight_'+weight_method]*df_MV[\"Ind_CAP_Weight\"]\n",
    "    #eq +short\n",
    "    elif weight_method == 'eq_short_index':\n",
    "        df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = cal_3group_weight_eq(df_MV,factor,upper_pec,lower_pec,longbig,ind=False,short_index=True)\n",
    "    #eq_ind + short\n",
    "    elif weight_method == 'eq_ind_short_index':  \n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                         ,upper_pec,lower_pec,longbig,ind=True\n",
    "                                                                         ,short_index=True)[0]) \n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                         ,upper_pec,lower_pec,longbig,ind=True\n",
    "                                                                         ,short_index=True)[1]) \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    \n",
    "    #做多指數區\n",
    "    #tri + long\n",
    "    elif weight_method == 'tri_long_index':\n",
    "        df_MV[factor+'_Weight_'+weight_method],df_MV[factor+'_Rank'] = cal_3group_weight_tri(df_MV,factor\n",
    "                                                                            ,upper_pec,lower_pec,longbig,long_index=True) \n",
    "    #tri_ind + long\n",
    "    elif weight_method == 'tri_ind_long_index': \n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                            ,upper_pec,lower_pec,longbig,long_index=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                            ,upper_pec,lower_pec,longbig,long_index=True)[1])\n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)\n",
    "        df_MV[factor+'_Rank']                  =  pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #tri_ind_cap + long\n",
    "    elif weight_method == 'tri_ind_cap_long_index':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                           ,upper_pec,lower_pec,longbig,long_index=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                           ,upper_pec,lower_pec,longbig,long_index=True)[1]) \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Rank']                  =  pd.Series(rank.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Weight_'+weight_method] =  df_MV[factor+'_Weight_'+weight_method]*df_MV[\"Ind_CAP_Weight\"]\n",
    "    #eq + long\n",
    "    elif weight_method == 'eq_long_index':\n",
    "        df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = cal_3group_weight_eq(df_MV,factor,upper_pec,lower_pec,longbig,ind=False,long_index=True) \n",
    "    #eq_ind + long  \n",
    "    elif weight_method == 'eq_ind_long_index':   \n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                         ,upper_pec,lower_pec,longbig,ind=True\n",
    "                                                                         , long_index=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                         ,upper_pec,lower_pec,longbig,ind=True\n",
    "                                                                         , long_index=True)[1])\n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)\n",
    "        df_MV[factor+'_Rank']                  =  pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    \n",
    "    ## 因子值加權\n",
    "    #tri\n",
    "    elif weight_method == 'tri_value':\n",
    "        df_MV[factor+'_Weight_'+weight_method],df_MV[factor+'_Rank'] = cal_3group_weight_tri(df_MV,factor,upper_pec,lower_pec,longbig,value=True)\n",
    "    #tri_ind\n",
    "    elif weight_method == 'tri_ind_value': \n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                           ,upper_pec,lower_pec,longbig,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                           ,upper_pec,lower_pec,longbig,value=True)[1])   \n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True)/N_Ind_Select)\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #tri_ind_cap    \n",
    "    elif weight_method == 'tri_ind_cap_value':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                            ,upper_pec,lower_pec,longbig,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                            ,upper_pec,lower_pec,longbig,value=True)[1])    \n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Weight_'+weight_method] = df_MV[factor+'_Weight_'+weight_method]*df_MV[\"Ind_CAP_Weight\"]\n",
    "    #eq\n",
    "    elif weight_method == 'eq_value':\n",
    "        df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = cal_3group_weight_eq(df_MV,factor,upper_pec,lower_pec,longbig,ind=False,value=True)\n",
    "    #eq_ind\n",
    "    elif weight_method == 'eq_ind_value':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                                            ,upper_pec,lower_pec,longbig\n",
    "                                                                                            ,ind=True,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                                            ,upper_pec,lower_pec,longbig\n",
    "                                                                                            ,ind=True,value=True)[1])\n",
    "        df_MV[factor+'_Weight_'+weight_method] = pd.Series(weight.reset_index(level=0, drop=True))/N_Ind_Select  \n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    \n",
    "    #放空指數區\n",
    "    #tri +short\n",
    "    elif weight_method == 'tri_short_index_value':\n",
    "        df_MV[factor+'_Weight_'+weight_method],df_MV[factor+'_Rank'] = cal_3group_weight_tri(df_MV,factor,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,short_index=True,value=True)\n",
    "    #tri_ind +short\n",
    "    elif weight_method == 'tri_ind_short_index_value' :\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,short_index=True,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,short_index=True,value=True)[1]) \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))/N_Ind_Select\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #tri_ind_cap +short    \n",
    "    elif weight_method == 'tri_ind_cap_short_index_value':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,short_index=True,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,short_index=True,value=True)[1])  \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Rank']                  =  pd.Series(rank.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Weight_'+weight_method] =  df_MV[factor+'_Weight_'+weight_method]*df_MV[\"Ind_CAP_Weight\"]\n",
    "    #eq + short\n",
    "    elif weight_method == 'eq_short_index_value':\n",
    "        df_MV[factor+'_Weight_'+weight_method],df_MV[factor+'_Rank'] = cal_3group_weight_eq(df_MV,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,ind=False,short_index=True,value=True)\n",
    "    #eq_ind + short\n",
    "    elif weight_method == 'eq_ind_short_index_value':  \n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,ind=True,short_index=True,value=True)[0]) \n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,ind=True,short_index=True,value=True)[1]) \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))/N_Ind_Select \n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    \n",
    "    #做多指數區\n",
    "    #tri + long\n",
    "    elif weight_method == 'tri_long_index_value':\n",
    "        df_MV[factor+'_Weight_'+weight_method],df_MV[factor+'_Rank'] = cal_3group_weight_tri(df_MV,factor,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,long_index=True,value=True) \n",
    "    #tri_ind + long\n",
    "    elif weight_method == 'tri_ind_long_index_value' :\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,long_index=True,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,long_index=True,value=True)[1]) \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))/N_Ind_Select\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #tri_ind_cap + long\n",
    "    elif weight_method == 'tri_ind_cap_long_index_value':\n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,long_index=True,value=True)[0]) \n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_tri(x,factor\n",
    "                                                                                             ,upper_pec,lower_pec,longbig\n",
    "                                                                                             ,long_index=True,value=True)[1])  \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))\n",
    "        df_MV[factor+'_Weight_'+weight_method] = df_MV[factor+'_Weight_'+weight_method]*df_MV[\"Ind_CAP_Weight\"]\n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    "    #eq + long\n",
    "    elif weight_method == 'eq_long_index_value':\n",
    "        df_MV[factor+'_Weight_'+weight_method],df_MV[factor+'_Rank'] = cal_3group_weight_eq(df_MV,factor,upper_pec,lower_pec,longbig,\n",
    "                                                                                              ind=False,long_index=True,value=True) \n",
    "    #eq_ind + long  \n",
    "    elif weight_method == 'eq_ind_long_index_value':   \n",
    "        weight = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor,upper_pec,lower_pec,longbig,\n",
    "                                                                                              ind=True,long_index=True,value=True)[0])\n",
    "        rank   = df_Ind.groupby('Ind',as_index=False).apply(lambda x : cal_3group_weight_eq(x,factor,upper_pec,lower_pec,longbig,\n",
    "                                                                                              ind=True,long_index=True,value=True)[1]) \n",
    "        df_MV[factor+'_Weight_'+weight_method] =  pd.Series(weight.reset_index(level=0, drop=True))/N_Ind_Select \n",
    "        df_MV[factor+'_Rank']                  = pd.Series(rank.reset_index(level=0, drop=True))\n",
    " \n",
    "    return df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Factor_args = {}\n",
    "#主要策略\n",
    "Factor_args['weight_method'] = ['eq', 'tri', 'eq_ind', 'tri_ind', 'tri_ind_cap']\n",
    "#考慮做多、放空指數\n",
    "Factor_args['weight_method'] = Factor_args['weight_method']+[x+'_short_index' for x in Factor_args['weight_method']]+[x+'_long_index' for x in Factor_args['weight_method']]\n",
    "#值加權\n",
    "Factor_args['weight_method'] = Factor_args['weight_method']+[x+'_value' for x in Factor_args['weight_method']]\n",
    "#因子\n",
    "Main_Factor_Name = 'PE'\n",
    "Main_Factor_Long = 0\n",
    "periods = 1\n",
    "#只取前後10%\n",
    "Factor_args['upper_lower_perc_eq'] = {\"upper\" : 0.1, \"lower\" : 0.9}\n",
    "#切一半\n",
    "Factor_args['upper_lower_perc_tri'] = {\"upper\" : 0.5, \"lower\" : 0.5}\n",
    "Factor_args[Main_Factor_Name] = { \"factors\" : [],\n",
    "                                  \"longbig\" : [],\n",
    "                                  \"periods\" : [] }\n",
    "\n",
    "Factor_args[Main_Factor_Name]['factors'] = [Main_Factor_Name ]\n",
    "#long bid = 0 -> #賣因子大的，買因子小的\n",
    "Factor_args[Main_Factor_Name]['longbig'] = [Main_Factor_Long]*len(Factor_args[Main_Factor_Name]['factors']) \n",
    "Factor_args[Main_Factor_Name]['periods'] = [periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight_method': ['eq', 'tri', 'eq_ind', 'tri_ind', 'tri_ind_cap', 'eq_short_index', 'tri_short_index', 'eq_ind_short_index', 'tri_ind_short_index', 'tri_ind_cap_short_index', 'eq_long_index', 'tri_long_index', 'eq_ind_long_index', 'tri_ind_long_index', 'tri_ind_cap_long_index', 'eq_value', 'tri_value', 'eq_ind_value', 'tri_ind_value', 'tri_ind_cap_value', 'eq_short_index_value', 'tri_short_index_value', 'eq_ind_short_index_value', 'tri_ind_short_index_value', 'tri_ind_cap_short_index_value', 'eq_long_index_value', 'tri_long_index_value', 'eq_ind_long_index_value', 'tri_ind_long_index_value', 'tri_ind_cap_long_index_value'], 'upper_lower_perc_eq': {'upper': 0.1, 'lower': 0.9}, 'upper_lower_perc_tri': {'upper': 0.5, 'lower': 0.5}, 'PE': {'factors': ['PE'], 'longbig': [0], 'periods': [1]}}\n"
     ]
    }
   ],
   "source": [
    "print(Factor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pd.read_csv('./Data/output/df_merge_1_new.txt', chunksize=10 ** 6)\n",
    "df_merge_final = pd.concat([x for x in reader], ignore_index=True)\n",
    "#df_merge_final.count()\n",
    "\n",
    "factor = Factor_args[Main_Factor_Name]['factors'][0]\n",
    "periods = Factor_args[Main_Factor_Name]['periods'][0]\n",
    "\n",
    "reader = pd.read_csv('./Data/output/df_merge_2.txt', chunksize=10 ** 6)\n",
    "df_factor = pd.concat([x for x in reader], ignore_index=True)\n",
    "df_merge_final = pd.merge(left=df_merge_final, right=df_factor[['Date','Code',factor]], on=['Date','Code'], how='left')\n",
    "#df_merge_final.count()\n",
    "del df_factor\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift 市值方便篩選成分股\n",
    "df_merge_final['CAP_t-1'] = df_merge_final.groupby('Code')['CAP'].shift(1)\n",
    "# main factor -> PE -> 計算T-1、T-2\n",
    "factor_list = Factor_args[Main_Factor_Name]['factors']\n",
    "for factor in factor_list :\n",
    "    df_merge_final[factor+'_t-1'] = df_merge_final.groupby('Code')[factor].shift(1)\n",
    "    df_merge_final[factor+'_t-2'] = df_merge_final.groupby('Code')[factor].shift(2)\n",
    "\n",
    "# shift 收盤價方便計算投組損益\n",
    "df_merge_final['Close_t+1']= df_merge_final.groupby('Code')['Close'].shift(-1)\n",
    "# 調整開始時間 -> 2007/01/04\n",
    "date_begin = np.array(df_merge_final['Date'].unique())[2]\n",
    "df_merge_final = df_merge_final[df_merge_final['Date'] >= date_begin]\n",
    "#以5000萬平均日成交金額為一篩選標準\n",
    "df_merge_final['Volumn_a_select'] = (df_merge_final['Volumn_a_mean'] >= 50000000)*1\n",
    "# 剔除Close , factor t-1 t-2 為na的資料   \n",
    "df_merge_final = df_merge_final[pd.isna(df_merge_final['Close'])==False]\n",
    "df_merge_final = df_merge_final[pd.isna(df_merge_final['CAP_t-1'])==False]\n",
    "df_merge_final = df_merge_final[pd.isna(df_merge_final[factor+'_t-1'])==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "2007    359\n",
      "2008    515\n",
      "2009    288\n",
      "2010    393\n",
      "2011    469\n",
      "2012    342\n",
      "2013    247\n",
      "2014    312\n",
      "2015    368\n",
      "2016    322\n",
      "2017    275\n",
      "2018    377\n",
      "2019    333\n",
      "Name: Code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "first_date = np.array(df_merge_final.groupby('Year')['Date'].min())\n",
    "#依照所有first date的資料，篩選出這些股票數據 -> True or False\n",
    "df_Year = df_merge_final[[x in first_date for x in df_merge_final['Date']]]\n",
    "df_Year = df_Year.sort_values(['Code','Year']).reset_index(drop=True)\n",
    "temp    = df_Year[df_Year['Volumn_a_select']==1]\n",
    "#count how many companies 平均日成交量 > 5000萬 every year\n",
    "print(temp.groupby('Year')['Code'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "2007    200\n",
      "2008    200\n",
      "2009    200\n",
      "2010    200\n",
      "2011    200\n",
      "2012    200\n",
      "2013    200\n",
      "2014    201\n",
      "2015    200\n",
      "2016    200\n",
      "2017    200\n",
      "2018    200\n",
      "2019    201\n",
      "Name: Code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#降序排名CAP(t-1)\n",
    "#再選取CAP(t-1)前200大的\n",
    "temp['MV_t-1_Rank'] = temp.groupby('Year')['CAP_t-1'].rank(ascending=False)\n",
    "temp['MV_Select']   = (temp['MV_t-1_Rank'] < 201)*1\n",
    "temp                = temp[['Code','Year','MV_Select','Volumn_a_select','CAP_t-1','Ind']]\n",
    "temp_m              = temp[temp['MV_Select']==1] \n",
    "print(temp_m.groupby('Year')['Code'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標注每年200檔是值最大的個股，數量前10大產業\n",
    "temp_m  = temp_m.sort_values(['Year','Ind','Code']).reset_index(drop=True)\n",
    "Ind_select = temp_m.groupby('Year').apply(lambda x : select_top10_ind(x)).tolist()\n",
    "Ind_select = [ x for y in Ind_select for x in y ]\n",
    "temp_m['Ind_Select'] = Ind_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "2007    155\n",
      "2008    149\n",
      "2009    143\n",
      "2010    151\n",
      "2011    147\n",
      "2012    137\n",
      "2013    144\n",
      "2014    142\n",
      "2015    145\n",
      "2016    149\n",
      "2017    155\n",
      "2018    151\n",
      "2019    148\n",
      "Name: Code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 各年度前10產業數量\n",
    "temp_i= temp_m [temp_m.Ind_Select ==1]\n",
    "print(temp_i.groupby('Year')['Code'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算10產業市值權重\n",
    "#依照year、ind分類，去作加總 / #依照年分去作加總 -> 權重\n",
    "Ind_Cap_Weight = temp_i.groupby(['Year','Ind'])['CAP_t-1'].sum() / temp_i.groupby(['Year'])['CAP_t-1'].sum()\n",
    "temp_m[\"Ind_CAP_Weight\"] = [ Ind_Cap_Weight[x][y] if z ==1 else 0 for x,y,z in \n",
    "                                 zip(temp_m[\"Year\"],temp_m[\"Ind\"],temp_m[\"Ind_Select\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_Year = pd.merge(left=df_merge_final, right=temp_m[['Year','Code','MV_Select','Ind_Select','Ind_CAP_Weight']]\n",
    "                         , on=['Year','Code'], how='left')\n",
    "\n",
    "del df_merge_final, temp_m\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volumn</th>\n",
       "      <th>Volumn_a_mean</th>\n",
       "      <th>CAP</th>\n",
       "      <th>Year</th>\n",
       "      <th>Ind</th>\n",
       "      <th>大盤指數</th>\n",
       "      <th>Rm</th>\n",
       "      <th>PE</th>\n",
       "      <th>CAP_t-1</th>\n",
       "      <th>PE_t-1</th>\n",
       "      <th>PE_t-2</th>\n",
       "      <th>Close_t+1</th>\n",
       "      <th>Volumn_a_select</th>\n",
       "      <th>MV_Select</th>\n",
       "      <th>Ind_Select</th>\n",
       "      <th>Ind_CAP_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1101</td>\n",
       "      <td>20070104</td>\n",
       "      <td>29.35</td>\n",
       "      <td>2318.0</td>\n",
       "      <td>262098608.0</td>\n",
       "      <td>922.1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9071.09</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>13.3</td>\n",
       "      <td>925.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>29.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1101</td>\n",
       "      <td>20070105</td>\n",
       "      <td>29.05</td>\n",
       "      <td>7197.0</td>\n",
       "      <td>261089444.0</td>\n",
       "      <td>912.6</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8957.97</td>\n",
       "      <td>-0.012470</td>\n",
       "      <td>13.2</td>\n",
       "      <td>922.1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>13.4</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1101</td>\n",
       "      <td>20070108</td>\n",
       "      <td>29.00</td>\n",
       "      <td>6579.0</td>\n",
       "      <td>260653208.0</td>\n",
       "      <td>911.1</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8844.95</td>\n",
       "      <td>-0.012617</td>\n",
       "      <td>13.2</td>\n",
       "      <td>912.6</td>\n",
       "      <td>13.2</td>\n",
       "      <td>13.3</td>\n",
       "      <td>30.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1101</td>\n",
       "      <td>20070109</td>\n",
       "      <td>30.25</td>\n",
       "      <td>10074.0</td>\n",
       "      <td>260225256.0</td>\n",
       "      <td>950.3</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8905.89</td>\n",
       "      <td>0.006890</td>\n",
       "      <td>13.7</td>\n",
       "      <td>911.1</td>\n",
       "      <td>13.2</td>\n",
       "      <td>13.2</td>\n",
       "      <td>29.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1101</td>\n",
       "      <td>20070110</td>\n",
       "      <td>29.75</td>\n",
       "      <td>13399.0</td>\n",
       "      <td>259865000.0</td>\n",
       "      <td>934.6</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8801.28</td>\n",
       "      <td>-0.011746</td>\n",
       "      <td>13.5</td>\n",
       "      <td>950.3</td>\n",
       "      <td>13.7</td>\n",
       "      <td>13.2</td>\n",
       "      <td>29.05</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3797850</td>\n",
       "      <td>9962</td>\n",
       "      <td>20190506</td>\n",
       "      <td>12.05</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2497884.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>2019</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19441.43</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>12.1</td>\n",
       "      <td>12.1</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3797851</td>\n",
       "      <td>9962</td>\n",
       "      <td>20190507</td>\n",
       "      <td>12.00</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2495608.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2019</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19602.03</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>12.1</td>\n",
       "      <td>12.1</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3797852</td>\n",
       "      <td>9962</td>\n",
       "      <td>20190508</td>\n",
       "      <td>12.00</td>\n",
       "      <td>153.0</td>\n",
       "      <td>2501720.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2019</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19488.95</td>\n",
       "      <td>-0.005769</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>11.90</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3797853</td>\n",
       "      <td>9962</td>\n",
       "      <td>20190509</td>\n",
       "      <td>11.90</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2503732.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>2019</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19149.90</td>\n",
       "      <td>-0.017397</td>\n",
       "      <td>11.9</td>\n",
       "      <td>10.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3797854</td>\n",
       "      <td>9962</td>\n",
       "      <td>20190510</td>\n",
       "      <td>11.80</td>\n",
       "      <td>217.0</td>\n",
       "      <td>2512788.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2019</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19113.00</td>\n",
       "      <td>-0.001927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.7</td>\n",
       "      <td>11.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.70</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3797855 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Code      Date  Close   Volumn  Volumn_a_mean    CAP  Year   Ind  \\\n",
       "0        1101  20070104  29.35   2318.0    262098608.0  922.1  2007   1.0   \n",
       "1        1101  20070105  29.05   7197.0    261089444.0  912.6  2007   1.0   \n",
       "2        1101  20070108  29.00   6579.0    260653208.0  911.1  2007   1.0   \n",
       "3        1101  20070109  30.25  10074.0    260225256.0  950.3  2007   1.0   \n",
       "4        1101  20070110  29.75  13399.0    259865000.0  934.6  2007   1.0   \n",
       "...       ...       ...    ...      ...            ...    ...   ...   ...   \n",
       "3797850  9962  20190506  12.05     85.0      2497884.0   10.9  2019  10.0   \n",
       "3797851  9962  20190507  12.00     32.0      2495608.0   10.8  2019  10.0   \n",
       "3797852  9962  20190508  12.00    153.0      2501720.0   10.8  2019  10.0   \n",
       "3797853  9962  20190509  11.90     86.0      2503732.0   10.7  2019  10.0   \n",
       "3797854  9962  20190510  11.80    217.0      2512788.0   10.6  2019  10.0   \n",
       "\n",
       "             大盤指數        Rm    PE  CAP_t-1  PE_t-1  PE_t-2  Close_t+1  \\\n",
       "0         9071.09  0.002174  13.3    925.2    13.4    13.5      29.05   \n",
       "1         8957.97 -0.012470  13.2    922.1    13.3    13.4      29.00   \n",
       "2         8844.95 -0.012617  13.2    912.6    13.2    13.3      30.25   \n",
       "3         8905.89  0.006890  13.7    911.1    13.2    13.2      29.75   \n",
       "4         8801.28 -0.011746  13.5    950.3    13.7    13.2      29.05   \n",
       "...           ...       ...   ...      ...     ...     ...        ...   \n",
       "3797850  19441.43 -0.017950  12.1     10.9    12.1    12.1      12.00   \n",
       "3797851  19602.03  0.008261  12.0     10.9    12.1    12.1      12.00   \n",
       "3797852  19488.95 -0.005769  12.0     10.8    12.0    12.1      11.90   \n",
       "3797853  19149.90 -0.017397  11.9     10.8    12.0    12.0      11.80   \n",
       "3797854  19113.00 -0.001927   NaN     10.7    11.9    12.0      11.70   \n",
       "\n",
       "         Volumn_a_select  MV_Select  Ind_Select  Ind_CAP_Weight  \n",
       "0                      1        1.0         0.0             0.0  \n",
       "1                      1        1.0         0.0             0.0  \n",
       "2                      1        1.0         0.0             0.0  \n",
       "3                      1        1.0         0.0             0.0  \n",
       "4                      1        1.0         0.0             0.0  \n",
       "...                  ...        ...         ...             ...  \n",
       "3797850                0        NaN         NaN             NaN  \n",
       "3797851                0        NaN         NaN             NaN  \n",
       "3797852                0        NaN         NaN             NaN  \n",
       "3797853                0        NaN         NaN             NaN  \n",
       "3797854                0        NaN         NaN             NaN  \n",
       "\n",
       "[3797855 rows x 19 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_method_list = Factor_args['weight_method']\n",
    "factor_list = Factor_args[Main_Factor_Name]['factors']\n",
    "longbig_list = Factor_args[Main_Factor_Name]['longbig']\n",
    "Date_list = np.sort(df_merge_Year['Date'].unique())\n",
    "\n",
    "factor  = factor_list[0]\n",
    "longbig = longbig_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eq\n",
      "tri\n",
      "eq_ind\n",
      "tri_ind\n",
      "tri_ind_cap\n",
      "eq_short_index\n",
      "tri_short_index\n",
      "eq_ind_short_index\n",
      "tri_ind_short_index\n",
      "tri_ind_cap_short_index\n",
      "eq_long_index\n",
      "tri_long_index\n",
      "eq_ind_long_index\n",
      "tri_ind_long_index\n",
      "tri_ind_cap_long_index\n",
      "eq_value\n",
      "tri_value\n",
      "eq_ind_value\n",
      "tri_ind_value\n",
      "tri_ind_cap_value\n",
      "eq_short_index_value\n",
      "tri_short_index_value\n",
      "eq_ind_short_index_value\n",
      "tri_ind_short_index_value\n",
      "tri_ind_cap_short_index_value\n",
      "eq_long_index_value\n",
      "tri_long_index_value\n",
      "eq_ind_long_index_value\n",
      "tri_ind_long_index_value\n",
      "tri_ind_cap_long_index_value\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(weight_method_list)):\n",
    "    weight_method   = weight_method_list[k]\n",
    "    print(weight_method)\n",
    "    initial_account = 1000000000\n",
    "    df_Final = pd.DataFrame()\n",
    "    need_col       = ['_Nstock_' ,'_Amount_','_Weight_']\n",
    "    need_col       = [factor+x+weight_method for x in need_col]\n",
    "    need_col_less1 = [x+'_t-1' for x in need_col]\n",
    "    #progress = ProgressBar()\n",
    "    for i in range(len(Date_list)-1):\n",
    "        # day 1\n",
    "        if i == 0:\n",
    "            Date   = Date_list[i]\n",
    "            df_MV  = df_merge_Year[df_merge_Year.Date == Date]\n",
    "            df_MV  = df_MV[df_MV.MV_Select == 1]\n",
    "            df_Ind = df_MV[df_MV.Ind_Select == 1]\n",
    "            \n",
    "            df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = Cal_Weight(weight_method,df_MV,df_Ind,factor,longbig,Factor_args)\n",
    "            df_MV = df_MV[['Code', 'Date','Close','Close_t+1',factor+'_t-1',factor+'_t-2',factor+'_Rank'\n",
    "                           ,factor+'_Weight_'+weight_method,'Rm']]\n",
    "            df_MV = df_MV[pd.isna(df_MV[factor+'_Weight_'+weight_method]) == False]\n",
    "            #去掉權重為0的\n",
    "            df_MV = df_MV[df_MV[factor+'_Weight_'+weight_method] != 0]\n",
    "            df_MV[factor+'_Nstock_'+weight_method], df_MV[factor+'_Amount_'+weight_method] = Cal_Amount(df_MV,factor,weight_method)\n",
    "            df_Final = df_MV\n",
    "        # day 2 ~\n",
    "        else:\n",
    "            df_less1 = df_MV\n",
    "            Date   = Date_list[i]\n",
    "            df_MV  = df_merge_Year[df_merge_Year.Date == Date]\n",
    "            df_MV  = df_MV[df_MV.MV_Select == 1]\n",
    "            df_Ind = df_MV[df_MV.Ind_Select == 1]\n",
    "            \n",
    "            df_MV[factor+'_Weight_'+weight_method], df_MV[factor+'_Rank'] = Cal_Weight(weight_method,df_MV,df_Ind,factor,longbig,Factor_args)\n",
    "            df_MV = df_MV[['Code', 'Date','Close','Close_t+1',factor+'_t-1',factor+'_t-2',factor+'_Rank'\n",
    "                           ,factor+'_Weight_'+weight_method,'Rm']]\n",
    "            df_MV = df_MV[pd.isna(df_MV[factor+'_Weight_'+weight_method]) == False]\n",
    "            df_MV = df_MV[df_MV[factor+'_Weight_'+weight_method] != 0]\n",
    "            df_MV[factor+'_Nstock_'+weight_method], df_MV[factor+'_Amount_'+weight_method] = Cal_Amount(df_MV,factor,weight_method)\n",
    "            #將t存為t-1\n",
    "            df_less1[need_col_less1+[factor+'_Rank_t-1','Close_t+1_less1']] = df_less1[need_col+[factor+'_Rank','Close_t+1']]\n",
    "            df_MV_m = pd.merge(left=df_MV,right=df_less1[need_col_less1+['Code',factor+'_Rank_t-1','Close_t+1_less1']]\n",
    "                               ,on=['Code'],how='outer')\n",
    "            df_MV_m['Date'] = df_MV_m['Date'].fillna(method='ffill')\n",
    "            df_MV_m['Close']= df_MV_m['Close'].fillna(df_MV_m['Close_t+1_less1'])\n",
    "            df_MV_m = df_MV_m.fillna(0)\n",
    "            #合併t、t-1\n",
    "            df_Final = pd.concat([df_Final,df_MV_m],axis=0)\n",
    "    if not os.path.exists('./Stats_Detail_New/multiway/factor'):\n",
    "        os.makedirs('./Stats_Detail_New/multiway/factor')\n",
    "    if not os.path.exists('./Weight/multiway/factor'):\n",
    "        os.makedirs('./Weight/multiway/factor')     \n",
    "    #output    \n",
    "    df_Final.to_csv('./Weight/multiway/factor/'+factor+'_Weight_'+weight_method+'.txt',index=False)\n",
    "    \n",
    "    #performance\n",
    "    df_performance = df_Final.reset_index(drop=True)\n",
    "    \n",
    "    df_Equity_Trunover_Tax = pd.DataFrame()\n",
    "    df_Equity_Trunover_Tax[['Date','Profit_'+weight_method]] = df_performance.groupby('Date').apply(lambda x : \n",
    "                                                                            Cal_Profit(x,factor ,weight_method)).reset_index()\n",
    "    df_Equity_Trunover_Tax['Tax_'+weight_method] = df_performance.groupby('Date').apply(lambda x : \n",
    "                                                                            Cal_Cost(x,factor ,weight_method)).reset_index(drop=True)\n",
    "    df_Equity_Trunover_Tax['Turnover_'+weight_method] = df_performance.groupby('Date').apply(lambda x : \n",
    "                                                                            Cal_Turnover(x,factor ,weight_method)).reset_index(drop=True)\n",
    "    df_Equity_Trunover_Tax['Equity_'+weight_method] = initial_account + df_Equity_Trunover_Tax['Profit_'+weight_method].cumsum() - df_Equity_Trunover_Tax['Tax_'+weight_method].cumsum()\n",
    "    df_Equity_Trunover_Tax.to_csv('./Stats_Detail_New/multiway/factor/'+factor+'_Stats_Detail_New_'+weight_method+'.txt',index=False)\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
